{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "d4f949aa7d0c47f99d62ef4d0a041b29": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_134e233cd4464c07964332abe48e87b5",
              "IPY_MODEL_57692d43b8f8437cb9abe57e9b5b2bc7",
              "IPY_MODEL_5a28c058680243f4b84232944fcda3c9"
            ],
            "layout": "IPY_MODEL_b77a2fee61ba4be8960c6ffbee0833f8"
          }
        },
        "134e233cd4464c07964332abe48e87b5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d8f001ab63b84603a47db37d73a99c0a",
            "placeholder": "​",
            "style": "IPY_MODEL_50a61b66a43648a5910e383d9a469970",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "57692d43b8f8437cb9abe57e9b5b2bc7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e2fafd51043942ab83f9ff9b1dbc08e6",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ec3cd3b2211543509ec944d638a8fdae",
            "value": 2
          }
        },
        "5a28c058680243f4b84232944fcda3c9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1f36feb2a79e4263a38629e9cb50fb63",
            "placeholder": "​",
            "style": "IPY_MODEL_3730571aaac44d98a5d31cb221b82bfa",
            "value": " 2/2 [00:31&lt;00:00, 15.20s/it]"
          }
        },
        "b77a2fee61ba4be8960c6ffbee0833f8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d8f001ab63b84603a47db37d73a99c0a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "50a61b66a43648a5910e383d9a469970": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e2fafd51043942ab83f9ff9b1dbc08e6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ec3cd3b2211543509ec944d638a8fdae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1f36feb2a79e4263a38629e9cb50fb63": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3730571aaac44d98a5d31cb221b82bfa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8db45ce5df694042bcbb979da1291d7b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8dbc7a1b584340e6993cc69ca4981b7d",
              "IPY_MODEL_6f27aaee417c42309accb3dc23f252bf",
              "IPY_MODEL_0068397c53bf457283a1c4b8e304ff42"
            ],
            "layout": "IPY_MODEL_977bbcf6dd7d4de2a343f87ed8bddd7f"
          }
        },
        "8dbc7a1b584340e6993cc69ca4981b7d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ece8d625de7842cfbb1f8618fdeed17e",
            "placeholder": "​",
            "style": "IPY_MODEL_43439d5caa434cf998fd60854d3d4061",
            "value": "Applying formatting function to train dataset: 100%"
          }
        },
        "6f27aaee417c42309accb3dc23f252bf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f447b5f8bf434a15944e9aec92efdb13",
            "max": 48,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_bf6b370531e04bbba736ea04d4a1ff4c",
            "value": 48
          }
        },
        "0068397c53bf457283a1c4b8e304ff42": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4377cdbc9aaa4874855a71372c3c916c",
            "placeholder": "​",
            "style": "IPY_MODEL_0ba6dcf626d041d4bc51ece6c92ded55",
            "value": " 48/48 [00:00&lt;00:00, 660.03 examples/s]"
          }
        },
        "977bbcf6dd7d4de2a343f87ed8bddd7f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ece8d625de7842cfbb1f8618fdeed17e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "43439d5caa434cf998fd60854d3d4061": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f447b5f8bf434a15944e9aec92efdb13": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bf6b370531e04bbba736ea04d4a1ff4c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4377cdbc9aaa4874855a71372c3c916c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0ba6dcf626d041d4bc51ece6c92ded55": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9196fda2d04c48e792dd509f9dbc8629": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c3724c46c9fc4deeb5c961bd0ead4f1c",
              "IPY_MODEL_f00f14c83f47477490ad70ea5eb2096d",
              "IPY_MODEL_8da35e769a7c4fe19e8a58ffc85c1899"
            ],
            "layout": "IPY_MODEL_1a480c604f9448d49db9334be9f906c5"
          }
        },
        "c3724c46c9fc4deeb5c961bd0ead4f1c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b6909efa4a2640ff90e01ec2a5472a98",
            "placeholder": "​",
            "style": "IPY_MODEL_4baca722df9d4a0e83d05b2affb7f1c4",
            "value": "Converting train dataset to ChatML: 100%"
          }
        },
        "f00f14c83f47477490ad70ea5eb2096d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4b6eb67dfa7248df921a972522d790fa",
            "max": 48,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d704a02854d244f3ad49264b2664a9a0",
            "value": 48
          }
        },
        "8da35e769a7c4fe19e8a58ffc85c1899": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b49b686d148b4e23b2ee3bc489ff605e",
            "placeholder": "​",
            "style": "IPY_MODEL_fa17a0e6d3dd4f61a10d5237424e826c",
            "value": " 48/48 [00:00&lt;00:00, 1365.04 examples/s]"
          }
        },
        "1a480c604f9448d49db9334be9f906c5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b6909efa4a2640ff90e01ec2a5472a98": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4baca722df9d4a0e83d05b2affb7f1c4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4b6eb67dfa7248df921a972522d790fa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d704a02854d244f3ad49264b2664a9a0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b49b686d148b4e23b2ee3bc489ff605e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fa17a0e6d3dd4f61a10d5237424e826c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d15717557abf44008484939fadfef7da": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0882963c868c4490af3cdea4a1078abe",
              "IPY_MODEL_12a73a51c79344de947d0944cd2aa07f",
              "IPY_MODEL_c754e23d85e942799ce59a3f34c96369"
            ],
            "layout": "IPY_MODEL_86b07979b60f4609ba831cfd81df6f64"
          }
        },
        "0882963c868c4490af3cdea4a1078abe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_102165d9fc6a4dbe82e53beba5c84161",
            "placeholder": "​",
            "style": "IPY_MODEL_b5a3d7186a42467dac3c9ef7b25f000b",
            "value": "Applying chat template to train dataset: 100%"
          }
        },
        "12a73a51c79344de947d0944cd2aa07f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7e6a11121fee4e0c837d4c7244a5e57b",
            "max": 48,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6fd91ce221944e1f9339ebce0da6baa3",
            "value": 48
          }
        },
        "c754e23d85e942799ce59a3f34c96369": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ed0f0b17e43d4e45bdfbd8db5a76f9da",
            "placeholder": "​",
            "style": "IPY_MODEL_0eb74671a57443f09c1d83236126b663",
            "value": " 48/48 [00:00&lt;00:00, 855.27 examples/s]"
          }
        },
        "86b07979b60f4609ba831cfd81df6f64": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "102165d9fc6a4dbe82e53beba5c84161": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b5a3d7186a42467dac3c9ef7b25f000b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7e6a11121fee4e0c837d4c7244a5e57b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6fd91ce221944e1f9339ebce0da6baa3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ed0f0b17e43d4e45bdfbd8db5a76f9da": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0eb74671a57443f09c1d83236126b663": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "98041a55e37c43c0ab7a9fb594c0eadd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9b69abd8d1354f4e8aafc701b79ed49a",
              "IPY_MODEL_7889885eeafd4a148d9d92fdc60f8517",
              "IPY_MODEL_94eda0db72c54937b46002d8eae6afa3"
            ],
            "layout": "IPY_MODEL_36d1a6347e6e4d5ca5d461f2d0085647"
          }
        },
        "9b69abd8d1354f4e8aafc701b79ed49a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1a29891131fd420395b508ce7f29448d",
            "placeholder": "​",
            "style": "IPY_MODEL_5f36479ab093452c8018c2b468f3acd4",
            "value": "Tokenizing train dataset: 100%"
          }
        },
        "7889885eeafd4a148d9d92fdc60f8517": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_94589cecec5f420a897122a53fb3e8b5",
            "max": 48,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_162c933690064727ba75ca1c496cc6d2",
            "value": 48
          }
        },
        "94eda0db72c54937b46002d8eae6afa3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c816fc7a6d9b47099cf1af5624af4b9f",
            "placeholder": "​",
            "style": "IPY_MODEL_d2a4892e529241218ed785ac173e26b0",
            "value": " 48/48 [00:00&lt;00:00, 477.88 examples/s]"
          }
        },
        "36d1a6347e6e4d5ca5d461f2d0085647": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1a29891131fd420395b508ce7f29448d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5f36479ab093452c8018c2b468f3acd4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "94589cecec5f420a897122a53fb3e8b5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "162c933690064727ba75ca1c496cc6d2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c816fc7a6d9b47099cf1af5624af4b9f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d2a4892e529241218ed785ac173e26b0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "593ceba5e8b1444da056644a8f494ca8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ff8f8c3f6e8d497591b561e996b7b606",
              "IPY_MODEL_e4dc09a7a8714bf6b3746d32272e9db5",
              "IPY_MODEL_8d9eb55f1c834d3180728823315f8194"
            ],
            "layout": "IPY_MODEL_5609682fe1334ebca13b18300513ad88"
          }
        },
        "ff8f8c3f6e8d497591b561e996b7b606": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ba25525cb71244d69db8c5f3bf72befe",
            "placeholder": "​",
            "style": "IPY_MODEL_2658848c017d4f42bf708ae1662ee2f7",
            "value": "Truncating train dataset: 100%"
          }
        },
        "e4dc09a7a8714bf6b3746d32272e9db5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ccefdd927de44628b196d17d58b798d4",
            "max": 48,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_03c440e97faf45e794d3220646e94368",
            "value": 48
          }
        },
        "8d9eb55f1c834d3180728823315f8194": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f487e61dbdbc4c698cc83a6d694bd6f5",
            "placeholder": "​",
            "style": "IPY_MODEL_7cec9de4cdbe4c049b73ebf176c3fce9",
            "value": " 48/48 [00:00&lt;00:00, 954.10 examples/s]"
          }
        },
        "5609682fe1334ebca13b18300513ad88": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ba25525cb71244d69db8c5f3bf72befe": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2658848c017d4f42bf708ae1662ee2f7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ccefdd927de44628b196d17d58b798d4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "03c440e97faf45e794d3220646e94368": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f487e61dbdbc4c698cc83a6d694bd6f5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7cec9de4cdbe4c049b73ebf176c3fce9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "160f850b209e405fae82e0dacfb966d8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_75cf109485bc4f2885b6d2e7e913e173",
              "IPY_MODEL_d5ddb3a6bb8c401ca6a33da5e1319bdd",
              "IPY_MODEL_f1bb03924ffd44f39db7a915606c3c53"
            ],
            "layout": "IPY_MODEL_bd3fd42da32f4c829a55304c41dbba99"
          }
        },
        "75cf109485bc4f2885b6d2e7e913e173": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_663b68517a6f4669872f966b7ea2e0ae",
            "placeholder": "​",
            "style": "IPY_MODEL_f39f2250b44d4dc9a6d5af4aefde50a2",
            "value": "Applying formatting function to eval dataset: 100%"
          }
        },
        "d5ddb3a6bb8c401ca6a33da5e1319bdd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a9d0b4c15dd84cc288e2d43efa1942a9",
            "max": 6,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a4ee3a98356b49d2a41c847bd3de3417",
            "value": 6
          }
        },
        "f1bb03924ffd44f39db7a915606c3c53": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b2981d7f5ee247559e0b7aba8d31ea23",
            "placeholder": "​",
            "style": "IPY_MODEL_d0216995a27b41538c7f742613a60e1e",
            "value": " 6/6 [00:00&lt;00:00, 239.89 examples/s]"
          }
        },
        "bd3fd42da32f4c829a55304c41dbba99": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "663b68517a6f4669872f966b7ea2e0ae": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f39f2250b44d4dc9a6d5af4aefde50a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a9d0b4c15dd84cc288e2d43efa1942a9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a4ee3a98356b49d2a41c847bd3de3417": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b2981d7f5ee247559e0b7aba8d31ea23": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d0216995a27b41538c7f742613a60e1e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "46c025e84d9c42e6895a5d63c6275028": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_18ae78bda19440359e579e4be6568fbc",
              "IPY_MODEL_7e2bbd9023b94e04b1a0c7f607762b42",
              "IPY_MODEL_ff8bc240dec044be88ac05f378304554"
            ],
            "layout": "IPY_MODEL_a1d9a313121e4889ad7fc7c23662bdef"
          }
        },
        "18ae78bda19440359e579e4be6568fbc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d8f767c6755044ba96ae6c004a783318",
            "placeholder": "​",
            "style": "IPY_MODEL_30319c20bab743d49b3d73fb84910fa4",
            "value": "Converting eval dataset to ChatML: 100%"
          }
        },
        "7e2bbd9023b94e04b1a0c7f607762b42": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_80a46ef9be524fafa2c7e4000538d611",
            "max": 6,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_82d5b43bb55e4314ac9ef49400fc7450",
            "value": 6
          }
        },
        "ff8bc240dec044be88ac05f378304554": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8af6fde012f14cc1bbb3c94e4a92f535",
            "placeholder": "​",
            "style": "IPY_MODEL_4886fd82c091414d8208e6574184e456",
            "value": " 6/6 [00:00&lt;00:00, 81.24 examples/s]"
          }
        },
        "a1d9a313121e4889ad7fc7c23662bdef": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d8f767c6755044ba96ae6c004a783318": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "30319c20bab743d49b3d73fb84910fa4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "80a46ef9be524fafa2c7e4000538d611": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "82d5b43bb55e4314ac9ef49400fc7450": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8af6fde012f14cc1bbb3c94e4a92f535": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4886fd82c091414d8208e6574184e456": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "dbdf845259f54d4eb824bb850892faf8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_75d61a2418894e9a88d137c8c48548d5",
              "IPY_MODEL_d9c9d645aa8d47eaa631ba2fe239305a",
              "IPY_MODEL_8d5199255a4f4f6d83538c3ab4ce218e"
            ],
            "layout": "IPY_MODEL_b0e732b38e9549a9ac018a099a79114b"
          }
        },
        "75d61a2418894e9a88d137c8c48548d5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3a8126d80c7a43fcb4f6c2b96669514e",
            "placeholder": "​",
            "style": "IPY_MODEL_ba51cc1e10ff475caba328a0754fe14c",
            "value": "Applying chat template to eval dataset: 100%"
          }
        },
        "d9c9d645aa8d47eaa631ba2fe239305a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cd773844411445ab8eb52915e5567794",
            "max": 6,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ee6d960179994311834a754aefb276d3",
            "value": 6
          }
        },
        "8d5199255a4f4f6d83538c3ab4ce218e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_55296c3406d14880a268388931fac89f",
            "placeholder": "​",
            "style": "IPY_MODEL_3ea6749c8b9c48399c70fe9aa082fbdc",
            "value": " 6/6 [00:00&lt;00:00, 246.40 examples/s]"
          }
        },
        "b0e732b38e9549a9ac018a099a79114b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3a8126d80c7a43fcb4f6c2b96669514e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ba51cc1e10ff475caba328a0754fe14c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cd773844411445ab8eb52915e5567794": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ee6d960179994311834a754aefb276d3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "55296c3406d14880a268388931fac89f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3ea6749c8b9c48399c70fe9aa082fbdc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "13abb4100532474d9bf8a995c1b77a45": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_583a68c063064fc482a26580b4fe5606",
              "IPY_MODEL_7cab7ba501d24a208cc0e2ce023ae7ae",
              "IPY_MODEL_43920e6d58df4799a1f8db41a892012e"
            ],
            "layout": "IPY_MODEL_b72c748d3a954a3da84de26c7b9c5db4"
          }
        },
        "583a68c063064fc482a26580b4fe5606": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1a4103e70da74f9dac391511521dcdd0",
            "placeholder": "​",
            "style": "IPY_MODEL_6e11b836e05e40b394a0e2dcbf1fc449",
            "value": "Tokenizing eval dataset: 100%"
          }
        },
        "7cab7ba501d24a208cc0e2ce023ae7ae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_673acbf7b3fd43e3881db7430ad09ced",
            "max": 6,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_819ec926b3494c89b8d9d13ace3a8d79",
            "value": 6
          }
        },
        "43920e6d58df4799a1f8db41a892012e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5c0ad9b71b9f42f5b05ae666082ce484",
            "placeholder": "​",
            "style": "IPY_MODEL_588681f0b95641ac9290c0728061cd6a",
            "value": " 6/6 [00:00&lt;00:00, 150.69 examples/s]"
          }
        },
        "b72c748d3a954a3da84de26c7b9c5db4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1a4103e70da74f9dac391511521dcdd0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6e11b836e05e40b394a0e2dcbf1fc449": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "673acbf7b3fd43e3881db7430ad09ced": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "819ec926b3494c89b8d9d13ace3a8d79": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5c0ad9b71b9f42f5b05ae666082ce484": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "588681f0b95641ac9290c0728061cd6a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "495cac7868c242f0afd7c3786eeb3880": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4c42ed4e465343f6a96d7af1f3ba50a1",
              "IPY_MODEL_a8b93ec0caa74c6a9b02624f47f39e7e",
              "IPY_MODEL_ce6dd6083bd942e89ecb44eb23927d96"
            ],
            "layout": "IPY_MODEL_e70a47ccab824306993780d522d65226"
          }
        },
        "4c42ed4e465343f6a96d7af1f3ba50a1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a566147680c94a1cbe3b4ea5d2e5a3ed",
            "placeholder": "​",
            "style": "IPY_MODEL_05f1ac8927284e19925b23ed1377cbfa",
            "value": "Truncating eval dataset: 100%"
          }
        },
        "a8b93ec0caa74c6a9b02624f47f39e7e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6ab3b2184fe04aa0bc50e7daf7e8ae8a",
            "max": 6,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_62cd50912ee742bd8c5aa623c5c62152",
            "value": 6
          }
        },
        "ce6dd6083bd942e89ecb44eb23927d96": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3efb446f1f914b50bc2267d85aba43c6",
            "placeholder": "​",
            "style": "IPY_MODEL_522d2e52a45d438c8b3ed0d204ad1114",
            "value": " 6/6 [00:00&lt;00:00, 244.83 examples/s]"
          }
        },
        "e70a47ccab824306993780d522d65226": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a566147680c94a1cbe3b4ea5d2e5a3ed": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "05f1ac8927284e19925b23ed1377cbfa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6ab3b2184fe04aa0bc50e7daf7e8ae8a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "62cd50912ee742bd8c5aa623c5c62152": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3efb446f1f914b50bc2267d85aba43c6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "522d2e52a45d438c8b3ed0d204ad1114": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Setting Up the Fine-tuning Environment"
      ],
      "metadata": {
        "id": "dGo-JeeRx5oC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets trl"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xZQ6kBTLyYES",
        "outputId": "856d7cec-45cc-44de-d9ba-dc199fdeefec"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.3.2)\n",
            "Requirement already satisfied: trl in /usr/local/lib/python3.11/dist-packages (0.15.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.17.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.10.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.13)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.28.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: accelerate>=0.34.0 in /usr/local/lib/python3.11/dist-packages (from trl) (1.4.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from trl) (13.9.4)\n",
            "Requirement already satisfied: transformers>=4.46.0 in /usr/local/lib/python3.11/dist-packages (from trl) (4.49.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.34.0->trl) (5.9.5)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.34.0->trl) (2.5.1+cu124)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.34.0->trl) (0.5.3)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.4.6)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.46.0->trl) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.46.0->trl) (0.21.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->trl) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->trl) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->trl) (0.1.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (3.1.5)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (12.4.127)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0.0->accelerate>=0.34.0->trl) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->accelerate>=0.34.0->trl) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 361
        },
        "id": "UypaIlYoxqah",
        "outputId": "0b71f4b2-e611-46f0-abfa-8c05ab8d29e5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmyeduwebsites\u001b[0m (\u001b[33mmyeduwebsites-university-of-moratuwa\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.7"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250309_224351-mpm4q11p</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/myeduwebsites-university-of-moratuwa/qwen-ai-research-qa/runs/mpm4q11p' target=\"_blank\">dazzling-shape-5</a></strong> to <a href='https://wandb.ai/myeduwebsites-university-of-moratuwa/qwen-ai-research-qa' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/myeduwebsites-university-of-moratuwa/qwen-ai-research-qa' target=\"_blank\">https://wandb.ai/myeduwebsites-university-of-moratuwa/qwen-ai-research-qa</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/myeduwebsites-university-of-moratuwa/qwen-ai-research-qa/runs/mpm4q11p' target=\"_blank\">https://wandb.ai/myeduwebsites-university-of-moratuwa/qwen-ai-research-qa/runs/mpm4q11p</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training dataset size: 48\n",
            "Validation dataset size: 6\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments\n",
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
        "from datasets import load_dataset\n",
        "from datasets import Dataset\n",
        "import json\n",
        "from trl import SFTTrainer\n",
        "import wandb\n",
        "\n",
        "# Initialize wandb for experiment tracking\n",
        "wandb.init(project=\"qwen-ai-research-qa\")\n",
        "\n",
        "# Check for GPU availability\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Load datasets\n",
        "def load_json_dataset(file_path):\n",
        "    with open(file_path, 'r') as f:\n",
        "        return json.load(f)\n",
        "\n",
        "train_data = load_json_dataset(\"/content/drive/MyDrive/Mirai - ML hackathon/Task3/train_data.json\")\n",
        "val_data = load_json_dataset(\"/content/drive/MyDrive/Mirai - ML hackathon/Task3/val_data.json\")\n",
        "\n",
        "# Convert to HF datasets format\n",
        "def convert_to_hf_dataset(data):\n",
        "    formatted_data = []\n",
        "    for item in data:\n",
        "        formatted_data.append({\n",
        "            \"messages\": item[\"conversations\"]\n",
        "        })\n",
        "    return formatted_data\n",
        "\n",
        "train_dataset = Dataset.from_list(convert_to_hf_dataset(train_data))\n",
        "val_dataset = Dataset.from_list(convert_to_hf_dataset(val_data))\n",
        "\n",
        "# Define training hyperparameters\n",
        "model_name = \"Qwen/Qwen2.5-3B-Instruct\"\n",
        "output_dir = \"/content/drive/MyDrive/Mirai - ML hackathon/Task3/qwen-ai-research-qa\"\n",
        "\n",
        "# QLoRA configuration\n",
        "lora_config = LoraConfig(\n",
        "    r=16,  # Rank\n",
        "    lora_alpha=32,  # Alpha scaling\n",
        "    lora_dropout=0.05,  # Dropout probability\n",
        "    bias=\"none\",  # Don't train bias\n",
        "    task_type=\"CAUSAL_LM\",  # Task type\n",
        "    target_modules=[\n",
        "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "        \"gate_proj\", \"up_proj\", \"down_proj\"\n",
        "    ]  # Attention modules to target\n",
        ")\n",
        "\n",
        "# Training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=output_dir,\n",
        "    per_device_train_batch_size=2,  # Reduced batch size\n",
        "    gradient_accumulation_steps=16,  # Increase to maintain effective batch size if needed\n",
        "    num_train_epochs=3,\n",
        "    gradient_checkpointing=True,\n",
        "    optim=\"paged_adamw_32bit\",\n",
        "    logging_steps=10,\n",
        "    save_strategy=\"epoch\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    learning_rate=2e-4,\n",
        "    fp16=False,  # Mixed precision training\n",
        "    warmup_ratio=0.03,\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    weight_decay=0.01,\n",
        "    report_to=\"wandb\",\n",
        "    # BF16 would be used if available on GPU\n",
        "    bf16=torch.cuda.is_bf16_supported(),\n",
        "    max_grad_norm=0.3,  # Gradient clipping\n",
        ")\n",
        "\n",
        "# Load tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# Function to print model size\n",
        "def print_trainable_parameters(model):\n",
        "    trainable_params = 0\n",
        "    all_params = 0\n",
        "    for _, param in model.named_parameters():\n",
        "        all_params += param.numel()\n",
        "        if param.requires_grad:\n",
        "            trainable_params += param.numel()\n",
        "    print(\n",
        "        f\"trainable params: {trainable_params} || all params: {all_params} || \"\n",
        "        f\"trainable%: {100 * trainable_params / all_params:.2f}\"\n",
        "    )\n",
        "\n",
        "# Save configuration\n",
        "config = {\n",
        "    \"model_name\": model_name,\n",
        "    \"lora_config\": {\n",
        "        \"r\": lora_config.r,\n",
        "        \"lora_alpha\": lora_config.lora_alpha,\n",
        "        \"lora_dropout\": lora_config.lora_dropout,\n",
        "        \"target_modules\": list(lora_config.target_modules)\n",
        "    },\n",
        "    \"training_args\": {\n",
        "        \"num_train_epochs\": training_args.num_train_epochs,\n",
        "        \"per_device_train_batch_size\": training_args.per_device_train_batch_size,\n",
        "        \"gradient_accumulation_steps\": training_args.gradient_accumulation_steps,\n",
        "        \"learning_rate\": training_args.learning_rate,\n",
        "        \"warmup_ratio\": training_args.warmup_ratio,\n",
        "        \"weight_decay\": training_args.weight_decay\n",
        "    }\n",
        "}\n",
        "\n",
        "with open(os.path.join(output_dir, \"training_config.json\"), \"w\") as f:\n",
        "    json.dump(config, f, indent=2)\n",
        "\n",
        "# Print dataset stats\n",
        "print(f\"Training dataset size: {len(train_dataset)}\")\n",
        "print(f\"Validation dataset size: {len(val_dataset)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fine-tuning the Qwen 2.5 3B Model"
      ],
      "metadata": {
        "id": "HBUuYXb517eu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U bitsandbytes transformers accelerate"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YYLaCwkC5MdO",
        "outputId": "ec744c69-bec7-465f-dc30-24bdc16c1268"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.11/dist-packages (0.45.3)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.49.0)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.4.0)\n",
            "Requirement already satisfied: torch<3,>=2.0 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (2.5.1+cu124)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (1.26.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.17.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.28.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (2024.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.1.5)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch<3,>=2.0->bitsandbytes) (1.3.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch<3,>=2.0->bitsandbytes) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Continuing from the setup code...\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments\n",
        "\n",
        "# Define training hyperparameters\n",
        "model_name = \"Qwen/Qwen2.5-3B-Instruct\"\n",
        "output_dir = \"/content/drive/MyDrive/Mirai - ML hackathon/Task3/qwen-ai-research-qa\"\n",
        "\n",
        "print(\"Loading model...\")\n",
        "# Load model in 4-bit quantization\n",
        "# model = AutoModelForCausalLM.from_pretrained(\n",
        "#     model_name,\n",
        "#     torch_dtype=torch.float16,\n",
        "#     load_in_4bit=True,\n",
        "#     device_map=\"auto\"\n",
        "# )\n",
        "from transformers import BitsAndBytesConfig\n",
        "\n",
        "# Define quantization config\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\"\n",
        ")\n",
        "\n",
        "# Load model with quantization config\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    torch_dtype=torch.float16,\n",
        "    quantization_config=bnb_config,  # Use the new argument\n",
        "    device_map=\"auto\",\n",
        "\n",
        ")\n",
        "# Prepare model for training\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "\n",
        "# Add LoRA adapters\n",
        "model = get_peft_model(model, lora_config)\n",
        "\n",
        "# Print trainable parameters\n",
        "print_trainable_parameters(model)\n",
        "\n",
        "# Chat template for proper formatting\n",
        "response_template = \"\\n<|im_start|>assistant\\n\"\n",
        "\n",
        "def formatting_func(example):\n",
        "    output_texts = []\n",
        "\n",
        "    # Check the structure of `example[\"messages\"]`\n",
        "    print(\"Debug:\", example[\"messages\"], type(example[\"messages\"]))\n",
        "\n",
        "    for messages in example[\"messages\"]:  # No need for indexing (i)\n",
        "        text = \"\"\n",
        "        if isinstance(messages, list):  # Ensure it's a list\n",
        "            for message in messages:\n",
        "                if isinstance(message, dict) and \"role\" in message and \"content\" in message:\n",
        "                    if message[\"role\"] == \"user\":\n",
        "                        text += f\"<|im_start|>user\\n{message['content']}<|im_end|>\\n\"\n",
        "                    else:\n",
        "                        text += f\"<|im_start|>assistant\\n{message['content']}<|im_end|>\\n\"\n",
        "                else:\n",
        "                    print(\"Warning: Unexpected message format:\", message)\n",
        "        else:\n",
        "            print(\"Warning: Unexpected messages format:\", messages)\n",
        "\n",
        "        output_texts.append(text)\n",
        "    print(output_texts)\n",
        "    return output_texts\n",
        "\n",
        "\n",
        "# Initialize SFT trainer\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    args=training_args,\n",
        "    tokenizer=tokenizer,\n",
        "    formatting_func=formatting_func,\n",
        "    peft_config=lora_config,\n",
        ")\n",
        "\n",
        "print(\"Starting training...\")\n",
        "trainer.train()\n",
        "\n",
        "# Save the trained model\n",
        "print(\"Saving model...\")\n",
        "trainer.save_model(output_dir)\n",
        "tokenizer.save_pretrained(output_dir)\n",
        "\n",
        "print(\"Training complete!\")\n",
        "\n",
        "# Evaluate the model's perplexity on the validation set\n",
        "eval_results = trainer.evaluate()\n",
        "print(f\"Validation Loss: {eval_results['eval_loss']}\")\n",
        "print(f\"Validation Perplexity: {torch.exp(torch.tensor(eval_results['eval_loss']))}\")\n",
        "\n",
        "# Log the final evaluation metrics to wandb\n",
        "wandb.log({\"final_val_loss\": eval_results[\"eval_loss\"],\n",
        "           \"final_val_perplexity\": torch.exp(torch.tensor(eval_results[\"eval_loss\"])).item()})\n",
        "wandb.finish()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "d4f949aa7d0c47f99d62ef4d0a041b29",
            "134e233cd4464c07964332abe48e87b5",
            "57692d43b8f8437cb9abe57e9b5b2bc7",
            "5a28c058680243f4b84232944fcda3c9",
            "b77a2fee61ba4be8960c6ffbee0833f8",
            "d8f001ab63b84603a47db37d73a99c0a",
            "50a61b66a43648a5910e383d9a469970",
            "e2fafd51043942ab83f9ff9b1dbc08e6",
            "ec3cd3b2211543509ec944d638a8fdae",
            "1f36feb2a79e4263a38629e9cb50fb63",
            "3730571aaac44d98a5d31cb221b82bfa",
            "8db45ce5df694042bcbb979da1291d7b",
            "8dbc7a1b584340e6993cc69ca4981b7d",
            "6f27aaee417c42309accb3dc23f252bf",
            "0068397c53bf457283a1c4b8e304ff42",
            "977bbcf6dd7d4de2a343f87ed8bddd7f",
            "ece8d625de7842cfbb1f8618fdeed17e",
            "43439d5caa434cf998fd60854d3d4061",
            "f447b5f8bf434a15944e9aec92efdb13",
            "bf6b370531e04bbba736ea04d4a1ff4c",
            "4377cdbc9aaa4874855a71372c3c916c",
            "0ba6dcf626d041d4bc51ece6c92ded55",
            "9196fda2d04c48e792dd509f9dbc8629",
            "c3724c46c9fc4deeb5c961bd0ead4f1c",
            "f00f14c83f47477490ad70ea5eb2096d",
            "8da35e769a7c4fe19e8a58ffc85c1899",
            "1a480c604f9448d49db9334be9f906c5",
            "b6909efa4a2640ff90e01ec2a5472a98",
            "4baca722df9d4a0e83d05b2affb7f1c4",
            "4b6eb67dfa7248df921a972522d790fa",
            "d704a02854d244f3ad49264b2664a9a0",
            "b49b686d148b4e23b2ee3bc489ff605e",
            "fa17a0e6d3dd4f61a10d5237424e826c",
            "d15717557abf44008484939fadfef7da",
            "0882963c868c4490af3cdea4a1078abe",
            "12a73a51c79344de947d0944cd2aa07f",
            "c754e23d85e942799ce59a3f34c96369",
            "86b07979b60f4609ba831cfd81df6f64",
            "102165d9fc6a4dbe82e53beba5c84161",
            "b5a3d7186a42467dac3c9ef7b25f000b",
            "7e6a11121fee4e0c837d4c7244a5e57b",
            "6fd91ce221944e1f9339ebce0da6baa3",
            "ed0f0b17e43d4e45bdfbd8db5a76f9da",
            "0eb74671a57443f09c1d83236126b663",
            "98041a55e37c43c0ab7a9fb594c0eadd",
            "9b69abd8d1354f4e8aafc701b79ed49a",
            "7889885eeafd4a148d9d92fdc60f8517",
            "94eda0db72c54937b46002d8eae6afa3",
            "36d1a6347e6e4d5ca5d461f2d0085647",
            "1a29891131fd420395b508ce7f29448d",
            "5f36479ab093452c8018c2b468f3acd4",
            "94589cecec5f420a897122a53fb3e8b5",
            "162c933690064727ba75ca1c496cc6d2",
            "c816fc7a6d9b47099cf1af5624af4b9f",
            "d2a4892e529241218ed785ac173e26b0",
            "593ceba5e8b1444da056644a8f494ca8",
            "ff8f8c3f6e8d497591b561e996b7b606",
            "e4dc09a7a8714bf6b3746d32272e9db5",
            "8d9eb55f1c834d3180728823315f8194",
            "5609682fe1334ebca13b18300513ad88",
            "ba25525cb71244d69db8c5f3bf72befe",
            "2658848c017d4f42bf708ae1662ee2f7",
            "ccefdd927de44628b196d17d58b798d4",
            "03c440e97faf45e794d3220646e94368",
            "f487e61dbdbc4c698cc83a6d694bd6f5",
            "7cec9de4cdbe4c049b73ebf176c3fce9",
            "160f850b209e405fae82e0dacfb966d8",
            "75cf109485bc4f2885b6d2e7e913e173",
            "d5ddb3a6bb8c401ca6a33da5e1319bdd",
            "f1bb03924ffd44f39db7a915606c3c53",
            "bd3fd42da32f4c829a55304c41dbba99",
            "663b68517a6f4669872f966b7ea2e0ae",
            "f39f2250b44d4dc9a6d5af4aefde50a2",
            "a9d0b4c15dd84cc288e2d43efa1942a9",
            "a4ee3a98356b49d2a41c847bd3de3417",
            "b2981d7f5ee247559e0b7aba8d31ea23",
            "d0216995a27b41538c7f742613a60e1e",
            "46c025e84d9c42e6895a5d63c6275028",
            "18ae78bda19440359e579e4be6568fbc",
            "7e2bbd9023b94e04b1a0c7f607762b42",
            "ff8bc240dec044be88ac05f378304554",
            "a1d9a313121e4889ad7fc7c23662bdef",
            "d8f767c6755044ba96ae6c004a783318",
            "30319c20bab743d49b3d73fb84910fa4",
            "80a46ef9be524fafa2c7e4000538d611",
            "82d5b43bb55e4314ac9ef49400fc7450",
            "8af6fde012f14cc1bbb3c94e4a92f535",
            "4886fd82c091414d8208e6574184e456",
            "dbdf845259f54d4eb824bb850892faf8",
            "75d61a2418894e9a88d137c8c48548d5",
            "d9c9d645aa8d47eaa631ba2fe239305a",
            "8d5199255a4f4f6d83538c3ab4ce218e",
            "b0e732b38e9549a9ac018a099a79114b",
            "3a8126d80c7a43fcb4f6c2b96669514e",
            "ba51cc1e10ff475caba328a0754fe14c",
            "cd773844411445ab8eb52915e5567794",
            "ee6d960179994311834a754aefb276d3",
            "55296c3406d14880a268388931fac89f",
            "3ea6749c8b9c48399c70fe9aa082fbdc",
            "13abb4100532474d9bf8a995c1b77a45",
            "583a68c063064fc482a26580b4fe5606",
            "7cab7ba501d24a208cc0e2ce023ae7ae",
            "43920e6d58df4799a1f8db41a892012e",
            "b72c748d3a954a3da84de26c7b9c5db4",
            "1a4103e70da74f9dac391511521dcdd0",
            "6e11b836e05e40b394a0e2dcbf1fc449",
            "673acbf7b3fd43e3881db7430ad09ced",
            "819ec926b3494c89b8d9d13ace3a8d79",
            "5c0ad9b71b9f42f5b05ae666082ce484",
            "588681f0b95641ac9290c0728061cd6a",
            "495cac7868c242f0afd7c3786eeb3880",
            "4c42ed4e465343f6a96d7af1f3ba50a1",
            "a8b93ec0caa74c6a9b02624f47f39e7e",
            "ce6dd6083bd942e89ecb44eb23927d96",
            "e70a47ccab824306993780d522d65226",
            "a566147680c94a1cbe3b4ea5d2e5a3ed",
            "05f1ac8927284e19925b23ed1377cbfa",
            "6ab3b2184fe04aa0bc50e7daf7e8ae8a",
            "62cd50912ee742bd8c5aa623c5c62152",
            "3efb446f1f914b50bc2267d85aba43c6",
            "522d2e52a45d438c8b3ed0d204ad1114"
          ]
        },
        "id": "_7FOmJf_2Eh0",
        "outputId": "e496bb50-17a6-4400-b43a-5215d5e45915"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading model...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d4f949aa7d0c47f99d62ef4d0a041b29"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainable params: 29933568 || all params: 1728606208 || trainable%: 1.73\n",
            "Debug: [{'content': 'Based on the DeepSeek V3 research paper, Could you elaborate on what is deepseek v3 and why is it significant in the ai industry?', 'role': 'user'}, {'content': 'DeepSeek V3 is an AI model from the Chinese startup Deepseek that has caused a stir in the AI space due to its impressive performance at a fraction of the training and inference cost compared to competitors. It uses a Mixture-of-Experts (MoE) architecture with 671B parameters but only 37B active parameters per token, which significantly reduces compute requirements. Its significance lies in improving algorithms at the software level rather than pushing for better hardware, making AI development more efficient and potentially disrupting the industry.', 'role': 'assistant'}] <class 'list'>\n",
            "Warning: Unexpected messages format: {'content': 'Based on the DeepSeek V3 research paper, Could you elaborate on what is deepseek v3 and why is it significant in the ai industry?', 'role': 'user'}\n",
            "Warning: Unexpected messages format: {'content': 'DeepSeek V3 is an AI model from the Chinese startup Deepseek that has caused a stir in the AI space due to its impressive performance at a fraction of the training and inference cost compared to competitors. It uses a Mixture-of-Experts (MoE) architecture with 671B parameters but only 37B active parameters per token, which significantly reduces compute requirements. Its significance lies in improving algorithms at the software level rather than pushing for better hardware, making AI development more efficient and potentially disrupting the industry.', 'role': 'assistant'}\n",
            "['', '']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-6-18900d270fb0>:72: FutureWarning: `tokenizer` is deprecated and removed starting from version 0.16.0 for `SFTTrainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = SFTTrainer(\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Applying formatting function to train dataset:   0%|          | 0/48 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8db45ce5df694042bcbb979da1291d7b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Debug: [[{'content': 'Based on the DeepSeek V3 research paper, Could you elaborate on what is deepseek v3 and why is it significant in the ai industry?', 'role': 'user'}, {'content': 'DeepSeek V3 is an AI model from the Chinese startup Deepseek that has caused a stir in the AI space due to its impressive performance at a fraction of the training and inference cost compared to competitors. It uses a Mixture-of-Experts (MoE) architecture with 671B parameters but only 37B active parameters per token, which significantly reduces compute requirements. Its significance lies in improving algorithms at the software level rather than pushing for better hardware, making AI development more efficient and potentially disrupting the industry.', 'role': 'assistant'}], [{'content': \"Based on the research about DeepSeek V3, please answer the following question: What are the key differences between DeepSeek's approach and that of other AI companies?\", 'role': 'user'}, {'content': \"The key difference in DeepSeek's approach compared to other AI companies is that while others pushed for better hardware, DeepSeek focused on improving algorithms, achieving better results at a software level. Specifically, they made training 45 times more efficient by: using 8-bit instead of 32-bit to save memory, compressing key value indices with 93% compression ratios, implementing multi-token prediction instead of single-token prediction (doubling inference speeds), and using a Mixture-of-Experts model that decomposes a big model into small models capable of running on consumer-grade hardware.\", 'role': 'assistant'}], [{'content': 'Based on the research about DeepSeek V3, please answer the following question: How many GPU hours did it take to train DeepSeek V3?', 'role': 'user'}, {'content': 'Training DeepSeek V3 took approximately 2.788 million GPU hours in total. This breaks down to 2,664,000 GPU hours for pre-training, 119,000 GPU hours for context length extension, and 5,000 GPU hours for post-training. The training was completed in less than two months using a cluster of 2,048 H800 GPUs.', 'role': 'assistant'}], [{'content': 'Based on the DeepSeek V3 research paper, What does the research say about what type of architecture does deepseek v3 use?', 'role': 'user'}, {'content': 'DeepSeek V3 uses a Mixture-of-Experts (MoE) architecture where only 37B parameters fire for each token out of the total 671B parameters. This sparse activation significantly reduces compute requirements compared to dense models. The model also uses Multi-head Latent Attention (MLA) which compresses the Key-Value cache, reducing memory usage and enabling more efficient training.', 'role': 'assistant'}], [{'content': \"Here is information about DeepSeek V3:\\n\\nFP8 Mixed Precision Training: They implemented an FP8 mixed precision training framework. Which reduces memory usage and accelerates training compared to higher precision formats. Reduced memory footprint by up to 50% compared to traditional FP16 FP32 formats. They use fine-grained quantisation strategies and increased accumulation precision to maintain accuracy.\\n\\nBased on this information, please answer: What was DeepSeek's approach to load balancing in their MoE architecture?\", 'role': 'user'}, {'content': 'DeepSeek pioneered an auxiliary loss-free strategy for load balancing in their Mixture-of-Experts (MoE) architecture. This improved performance without the drawbacks of traditional auxiliary loss methods that are typically used in MoE architectures.', 'role': 'assistant'}], [{'content': 'Based on the DeepSeek V3 research paper, Please explain how does deepseek v3 perform at reasoning and math compared to other models.', 'role': 'user'}, {'content': 'According to the document, DeepSeek V3 excels at reasoning and math tasks, surpassing the performance of GPT-4 and Claude 3.5 Sonnet in these areas. For writing and coding tasks, however, Claude 3.5 Sonnet maintains a slight lead over DeepSeek V3.', 'role': 'assistant'}], [{'content': 'Based on the DeepSeek V3 research paper, What information do you have on how many gpu hours did it take to train deepseek v3?', 'role': 'user'}, {'content': 'Training DeepSeek V3 took approximately 2.788 million GPU hours in total. This breaks down to 2,664,000 GPU hours for pre-training, 119,000 GPU hours for context length extension, and 5,000 GPU hours for post-training. The training was completed in less than two months using a cluster of 2,048 H800 GPUs.', 'role': 'assistant'}], [{'content': 'Based on the DeepSeek V3 research paper, What does the research say about what was the approximate cost of training deepseek v3 and how does it compare to other models?', 'role': 'user'}, {'content': \"The approximate cost of training DeepSeek V3 was around $5.5-5.6 million, which included approximately 2.788 million GPU hours at an estimated rate of $2 per GPU hour. The training used a cluster of 2,048 H800 GPUs and was completed in less than two months. This is significantly more efficient than competitor models like Llama 3.1, which reportedly required 30.84 million GPU hours for training on a similar amount of data (15 trillion tokens vs. DeepSeek's 14.8 trillion tokens).\", 'role': 'assistant'}], [{'content': 'Based on the DeepSeek V3 research paper, What information do you have on what compression ratio did deepseek achieve for key value indices?', 'role': 'user'}, {'content': 'According to the document, DeepSeek compressed key value indices which consume significant VRAM, achieving 93% compression ratios. This was part of their overall approach to make training more efficient.', 'role': 'assistant'}], [{'content': 'Based on the DeepSeek V3 research paper, What information do you have on how many tokens was deepseek v3 trained on?', 'role': 'user'}, {'content': \"DeepSeek V3 was trained on approximately 14.8 trillion high-quality tokens. For perspective, this is comparable to Llama 3.1's 15 trillion tokens, but DeepSeek achieved much greater efficiency in its training process.\", 'role': 'assistant'}], [{'content': 'Based on the DeepSeek V3 research paper, What information do you have on how many parameters does deepseek v3 have and how many are active during inference?', 'role': 'user'}, {'content': 'DeepSeek V3 has a total of 671B parameters in its Mixture-of-Experts (MoE) architecture, but only 37B parameters are active (fire) for each token during processing. This sparse activation approach significantly reduces the computational requirements compared to dense models of similar size.', 'role': 'assistant'}], [{'content': 'Based on the DeepSeek V3 research paper, What information do you have on what is multi-head latent attention (mla) and how was it used in deepseek v3?', 'role': 'user'}, {'content': 'Multi-head Latent Attention (MLA) is a technique used in DeepSeek V3 that compresses the Key-Value cache. This reduces memory usage and enables more efficient training by decreasing the memory footprint required during model operation.', 'role': 'assistant'}], [{'content': 'Based on the research about DeepSeek V3, please answer the following question: What is Multi-head Latent Attention (MLA) and how was it used in DeepSeek V3?', 'role': 'user'}, {'content': 'Multi-head Latent Attention (MLA) is a technique used in DeepSeek V3 that compresses the Key-Value cache. This reduces memory usage and enables more efficient training by decreasing the memory footprint required during model operation.', 'role': 'assistant'}], [{'content': \"Based on the DeepSeek V3 research paper, What information do you have on what was deepseek's approach to load balancing in their moe architecture?\", 'role': 'user'}, {'content': 'DeepSeek pioneered an auxiliary loss-free strategy for load balancing in their Mixture-of-Experts (MoE) architecture. This improved performance without the drawbacks of traditional auxiliary loss methods that are typically used in MoE architectures.', 'role': 'assistant'}], [{'content': 'Based on the DeepSeek V3 research paper, What information do you have on what type of architecture does deepseek v3 use?', 'role': 'user'}, {'content': 'DeepSeek V3 uses a Mixture-of-Experts (MoE) architecture where only 37B parameters fire for each token out of the total 671B parameters. This sparse activation significantly reduces compute requirements compared to dense models. The model also uses Multi-head Latent Attention (MLA) which compresses the Key-Value cache, reducing memory usage and enabling more efficient training.', 'role': 'assistant'}], [{'content': 'Based on the research about DeepSeek V3, please answer the following question: What type of architecture does DeepSeek V3 use?', 'role': 'user'}, {'content': 'DeepSeek V3 uses a Mixture-of-Experts (MoE) architecture where only 37B parameters fire for each token out of the total 671B parameters. This sparse activation significantly reduces compute requirements compared to dense models. The model also uses Multi-head Latent Attention (MLA) which compresses the Key-Value cache, reducing memory usage and enabling more efficient training.', 'role': 'assistant'}], [{'content': 'Based on the DeepSeek V3 research paper, What information do you have on how might the techniques used in deepseek v3 be applied to smaller models or different domains?', 'role': 'user'}, {'content': 'The techniques used in DeepSeek V3 could be applied to smaller models or different domains in several ways: 1) The Mixture-of-Experts architecture could help create more efficient smaller models by activating only relevant parameters, 2) FP8 mixed precision training could reduce memory requirements for models of any size, 3) The Key-Value cache compression techniques could improve efficiency in models designed for limited hardware, 4) Multi-token prediction could speed up inference across various applications, and 5) The custom training framework optimizations could benefit any distributed training setup regardless of model size or application domain.', 'role': 'assistant'}], [{'content': 'Based on the DeepSeek V3 research paper, Please explain what is deepseek v3 and why is it significant in the ai industry.', 'role': 'user'}, {'content': 'DeepSeek V3 is an AI model from the Chinese startup Deepseek that has caused a stir in the AI space due to its impressive performance at a fraction of the training and inference cost compared to competitors. It uses a Mixture-of-Experts (MoE) architecture with 671B parameters but only 37B active parameters per token, which significantly reduces compute requirements. Its significance lies in improving algorithms at the software level rather than pushing for better hardware, making AI development more efficient and potentially disrupting the industry.', 'role': 'assistant'}], [{'content': 'Here is information about DeepSeek V3:\\n\\nBreakdown of the costs of the Deepseek v3 model Deepseek\\'s flagship model v3 showcases an architecture with a 671B parameter MOE (Mixture of Agents) with 37B active parameters per token Their success stems from breakthrough engineering: using MoE architecture, implementing FP8 mixed precision training, and developing a custom HAI-LLM framework. Deepseek excels at reasoning and math, surpassing GPT-4 and Claude 3.5 Sonnet. For writing and coding tasks, Claude 3.5 Sonnet maintains a slight lead. Deepseek pre-trained this model on 14.8 trillion high-quality data, taking 2,788,000 GPU hours on the Nvidia h800s cluster, costing around only $6 million the Llama 403b was trained on 11x of that, taking 30,840,000 GPU hours, also on 15 trillion tokens. So how true is the claim of $5.5 million, or is it another marketing trick? Underlying FLOP calculations Model Details: Active Parameters: 37B (using FP8 precision) FLOPs per token: Using the rule of thumb \"6 FLOPs per parameter per token.\" 37B 6   222B FLOPs per token Total Training Tokens: Approximately 14.8 trillion tokens Total FLOPs required: 222 B FLOPs token 14.8 T tokens   3.3 10²⁴ FLOPs GPU FLOP Capacity (H800 H100): An H100 is roughly estimated to deliver about. 3.958 10¹⁵ FLOPs (per second or per some standardised interval   here used as a comparative metric). Ideal (Perfect Efficiency) GPU hours. (Dividing total required FLOPs by per GPU capability gives) 3.3 10²⁴   3.958 10¹⁵    8.33 10⁸ seconds  0.4 million GPU hour Note: This \"perfect efficiency\" scenario is a lower bound. Real-world training is less efficient. 2. Adjusting for Real World Inefficiencies (Comparison with Llama 3.1) Reference Model: Llama 3.1 (405B parameters, 15 T tokens) reportedly required 30.84 M GPU hours in practice. Recalculating FLOPs for Llama 3.1: Using the same math: 3.64 10²⁵ FLOPs required Scaling Efficiency Using the ratio of FLOPs needed for DeepSeek V3 versus Llama 3.1. and assuming similar inefficiencies. The estimate adjusts to roughly 2.79M GPU hours for DeepSeek V3 training. 3. DeepSeek V3 Reported Training Breakdown According to the DeepSeek V3 paper Pre training Stage: Per Trillion Tokens: 180K H800 GPU hours Overall Pre training: Total of 2,664K GPU hours This stage was completed in less than two months using a cluster of 2,048 H800 GPUs. Context Length Extension: Additional 119K GPU hours Post training: An extra 5K GPU hours Total GPU Hours: 2,664 K 119 K 5 K 2.788M GPU hours Cost Estimation Assumed GPU Rental Price: $2 per GPU hour Total Rental Cost: 2.788M GPU hours $2 hour $5.576 million as stated in Deepseek paper During the pre training stage, training DeepSeek V3 on each trillion tokens requires only 180K H800 GPU hours  Consequently, our pre training stage is completed in less than two months and costs 2664K GPU hours. Combined with 119K GPU hours for the context length extension and 5K GPU hours for post training, DeepSeek V3 costs only 2.788M GPU hours for its full training. Assuming the rental price of the H800 GPU is $2 per GPU hour, our total training costs amount to only $5.576M. Summary Theoretical (Perfect Efficiency) Estimate:  0.4 M GPU hours (using idealised FLOP counts and assuming perfect hardware utilisation0 Adjusted (Real World) Estimate (via Llama 3.1 comparison):  2.79 GPU hours DeepSeek V3 Reported Breakdown: Pre training: 2,664K GPU hours Context Extension: 119K GPU hours Post training: 5K GPU hours Total:  2.788 M GPU hours Cost (at $2 per GPU hour):  $5.576 million\\n\\nBased on this information, please answer: How does the training efficiency of DeepSeek V3 compare to Llama 3.1?', 'role': 'user'}, {'content': 'DeepSeek V3 was significantly more efficient in training compared to Llama 3.1. While DeepSeek V3 required about 2.788 million GPU hours to train on 14.8 trillion tokens, Llama 3.1 reportedly required 30.84 million GPU hours to train on a similar amount of data (15 trillion tokens). This means DeepSeek V3 was approximately 11 times more efficient than Llama 3.1 in terms of GPU hours required for training.', 'role': 'assistant'}], [{'content': 'Based on the research about DeepSeek V3, please answer the following question: How did DeepSeek V3 achieve training efficiency compared to other large language models?', 'role': 'user'}, {'content': 'DeepSeek V3 achieved significant training efficiency through several innovations: 1) Using a Mixture-of-Experts (MoE) architecture where only 37B parameters fire for each token out of the total 671B, 2) Implementing FP8 mixed precision training which reduced memory usage by up to 50% compared to traditional formats, 3) Developing a custom training framework called HAI-LLM with optimizations like the DualPipe algorithm for efficient pipeline parallelism, 4) Using Multi-head Latent Attention (MLA) to compress the Key-Value cache, 5) Pioneering an auxiliary loss-free strategy for load balancing in the MoE architecture, and 6) Implementing efficient cross-node communication kernels and memory optimizations.', 'role': 'assistant'}], [{'content': 'Based on the DeepSeek V3 research paper, Please explain what compression ratio did deepseek achieve for key value indices.', 'role': 'user'}, {'content': 'According to the document, DeepSeek compressed key value indices which consume significant VRAM, achieving 93% compression ratios. This was part of their overall approach to make training more efficient.', 'role': 'assistant'}], [{'content': 'Based on the DeepSeek V3 research paper, Could you elaborate on how does the training efficiency of deepseek v3 compare to llama 3.1?', 'role': 'user'}, {'content': 'DeepSeek V3 was significantly more efficient in training compared to Llama 3.1. While DeepSeek V3 required about 2.788 million GPU hours to train on 14.8 trillion tokens, Llama 3.1 reportedly required 30.84 million GPU hours to train on a similar amount of data (15 trillion tokens). This means DeepSeek V3 was approximately 11 times more efficient than Llama 3.1 in terms of GPU hours required for training.', 'role': 'assistant'}], [{'content': 'Here is information about DeepSeek V3:\\n\\nModel Architecture The model employs a Mixture-of-Experts (MoE) architecture, where only 37B parameters fire for each token out of the total 671B. This sparse activation significantly reduces compute requirements compared to dense models. The model uses Multi-head Latent Attention (MLA). This compresses the Key-Value cache, reducing memory usage and enabling more efficient training.\\n\\nBased on this information, please answer: What type of architecture does DeepSeek V3 use?', 'role': 'user'}, {'content': 'DeepSeek V3 uses a Mixture-of-Experts (MoE) architecture where only 37B parameters fire for each token out of the total 671B parameters. This sparse activation significantly reduces compute requirements compared to dense models. The model also uses Multi-head Latent Attention (MLA) which compresses the Key-Value cache, reducing memory usage and enabling more efficient training.', 'role': 'assistant'}], [{'content': 'Based on the DeepSeek V3 research paper, Can you tell me about how did deepseek v3 achieve training efficiency compared to other large language models?', 'role': 'user'}, {'content': 'DeepSeek V3 achieved significant training efficiency through several innovations: 1) Using a Mixture-of-Experts (MoE) architecture where only 37B parameters fire for each token out of the total 671B, 2) Implementing FP8 mixed precision training which reduced memory usage by up to 50% compared to traditional formats, 3) Developing a custom training framework called HAI-LLM with optimizations like the DualPipe algorithm for efficient pipeline parallelism, 4) Using Multi-head Latent Attention (MLA) to compress the Key-Value cache, 5) Pioneering an auxiliary loss-free strategy for load balancing in the MoE architecture, and 6) Implementing efficient cross-node communication kernels and memory optimizations.', 'role': 'assistant'}], [{'content': 'Here is information about DeepSeek V3:\\n\\nModel Architecture The model employs a Mixture-of-Experts (MoE) architecture, where only 37B parameters fire for each token out of the total 671B. This sparse activation significantly reduces compute requirements compared to dense models. The model uses Multi-head Latent Attention (MLA). This compresses the Key-Value cache, reducing memory usage and enabling more efficient training.\\n\\nBased on this information, please answer: How might the techniques used in DeepSeek V3 be applied to smaller models or different domains?', 'role': 'user'}, {'content': 'The techniques used in DeepSeek V3 could be applied to smaller models or different domains in several ways: 1) The Mixture-of-Experts architecture could help create more efficient smaller models by activating only relevant parameters, 2) FP8 mixed precision training could reduce memory requirements for models of any size, 3) The Key-Value cache compression techniques could improve efficiency in models designed for limited hardware, 4) Multi-token prediction could speed up inference across various applications, and 5) The custom training framework optimizations could benefit any distributed training setup regardless of model size or application domain.', 'role': 'assistant'}], [{'content': 'Based on the DeepSeek V3 research paper, Could you elaborate on how might the techniques used in deepseek v3 be applied to smaller models or different domains?', 'role': 'user'}, {'content': 'The techniques used in DeepSeek V3 could be applied to smaller models or different domains in several ways: 1) The Mixture-of-Experts architecture could help create more efficient smaller models by activating only relevant parameters, 2) FP8 mixed precision training could reduce memory requirements for models of any size, 3) The Key-Value cache compression techniques could improve efficiency in models designed for limited hardware, 4) Multi-token prediction could speed up inference across various applications, and 5) The custom training framework optimizations could benefit any distributed training setup regardless of model size or application domain.', 'role': 'assistant'}], [{'content': \"Based on the DeepSeek V3 research paper, I'd like to know more about what precision format did deepseek v3 use for training and what were its benefits.\", 'role': 'user'}, {'content': 'DeepSeek V3 implemented an FP8 mixed precision training framework, which reduced memory usage and accelerated training compared to higher precision formats. This approach reduced the memory footprint by up to 50% compared to traditional FP16/FP32 formats. They used fine-grained quantization strategies and increased accumulation precision to maintain accuracy while gaining the efficiency benefits.', 'role': 'assistant'}], [{'content': \"Based on the DeepSeek V3 research paper, Please explain what potential impact could deepseek v3's efficient approach have on the ai industry.\", 'role': 'user'}, {'content': \"DeepSeek V3's efficient approach could have several significant impacts on the AI industry: 1) It could democratize access to powerful AI by reducing the hardware requirements and costs, 2) It could accelerate AI development by enabling faster training cycles, 3) It could shift industry focus from hardware scaling to algorithmic efficiency, 4) It might lead to more environmentally sustainable AI by reducing energy consumption, and 5) It could disrupt the market dominance of major US tech companies, as evidenced by the market concerns and impact on share prices mentioned in the document.\", 'role': 'assistant'}], [{'content': 'Based on the DeepSeek V3 research paper, What information do you have on what precision format did deepseek v3 use for training and what were its benefits?', 'role': 'user'}, {'content': 'DeepSeek V3 implemented an FP8 mixed precision training framework, which reduced memory usage and accelerated training compared to higher precision formats. This approach reduced the memory footprint by up to 50% compared to traditional FP16/FP32 formats. They used fine-grained quantization strategies and increased accumulation precision to maintain accuracy while gaining the efficiency benefits.', 'role': 'assistant'}], [{'content': 'Here is information about DeepSeek V3:\\n\\nModel Architecture The model employs a Mixture-of-Experts (MoE) architecture, where only 37B parameters fire for each token out of the total 671B. This sparse activation significantly reduces compute requirements compared to dense models. The model uses Multi-head Latent Attention (MLA). This compresses the Key-Value cache, reducing memory usage and enabling more efficient training.\\n\\nBased on this information, please answer: How many parameters does DeepSeek V3 have and how many are active during inference?', 'role': 'user'}, {'content': 'DeepSeek V3 has a total of 671B parameters in its Mixture-of-Experts (MoE) architecture, but only 37B parameters are active (fire) for each token during processing. This sparse activation approach significantly reduces the computational requirements compared to dense models of similar size.', 'role': 'assistant'}], [{'content': 'Here is information about DeepSeek V3:\\n\\nFP8 Mixed Precision Training: They implemented an FP8 mixed precision training framework. Which reduces memory usage and accelerates training compared to higher precision formats. Reduced memory footprint by up to 50% compared to traditional FP16 FP32 formats. They use fine-grained quantisation strategies and increased accumulation precision to maintain accuracy.\\n\\nBased on this information, please answer: What precision format did DeepSeek V3 use for training and what were its benefits?', 'role': 'user'}, {'content': 'DeepSeek V3 implemented an FP8 mixed precision training framework, which reduced memory usage and accelerated training compared to higher precision formats. This approach reduced the memory footprint by up to 50% compared to traditional FP16/FP32 formats. They used fine-grained quantization strategies and increased accumulation precision to maintain accuracy while gaining the efficiency benefits.', 'role': 'assistant'}], [{'content': \"Based on the DeepSeek V3 research paper, Please explain what potential impact could deepseek v3's efficient approach have on the ai industry.\", 'role': 'user'}, {'content': \"DeepSeek V3's efficient approach could have several significant impacts on the AI industry: 1) It could democratize access to powerful AI by reducing the hardware requirements and costs, 2) It could accelerate AI development by enabling faster training cycles, 3) It could shift industry focus from hardware scaling to algorithmic efficiency, 4) It might lead to more environmentally sustainable AI by reducing energy consumption, and 5) It could disrupt the market dominance of major US tech companies, as evidenced by the market concerns and impact on share prices mentioned in the document.\", 'role': 'assistant'}], [{'content': 'Based on the DeepSeek V3 research paper, Could you elaborate on how did deepseek v3 achieve training efficiency compared to other large language models?', 'role': 'user'}, {'content': 'DeepSeek V3 achieved significant training efficiency through several innovations: 1) Using a Mixture-of-Experts (MoE) architecture where only 37B parameters fire for each token out of the total 671B, 2) Implementing FP8 mixed precision training which reduced memory usage by up to 50% compared to traditional formats, 3) Developing a custom training framework called HAI-LLM with optimizations like the DualPipe algorithm for efficient pipeline parallelism, 4) Using Multi-head Latent Attention (MLA) to compress the Key-Value cache, 5) Pioneering an auxiliary loss-free strategy for load balancing in the MoE architecture, and 6) Implementing efficient cross-node communication kernels and memory optimizations.', 'role': 'assistant'}], [{'content': 'Here is information about DeepSeek V3:\\n\\nBreakdown of the costs of the Deepseek v3 model Deepseek\\'s flagship model v3 showcases an architecture with a 671B parameter MOE (Mixture of Agents) with 37B active parameters per token Their success stems from breakthrough engineering: using MoE architecture, implementing FP8 mixed precision training, and developing a custom HAI-LLM framework. Deepseek excels at reasoning and math, surpassing GPT-4 and Claude 3.5 Sonnet. For writing and coding tasks, Claude 3.5 Sonnet maintains a slight lead. Deepseek pre-trained this model on 14.8 trillion high-quality data, taking 2,788,000 GPU hours on the Nvidia h800s cluster, costing around only $6 million the Llama 403b was trained on 11x of that, taking 30,840,000 GPU hours, also on 15 trillion tokens. So how true is the claim of $5.5 million, or is it another marketing trick? Underlying FLOP calculations Model Details: Active Parameters: 37B (using FP8 precision) FLOPs per token: Using the rule of thumb \"6 FLOPs per parameter per token.\" 37B 6   222B FLOPs per token Total Training Tokens: Approximately 14.8 trillion tokens Total FLOPs required: 222 B FLOPs token 14.8 T tokens   3.3 10²⁴ FLOPs GPU FLOP Capacity (H800 H100): An H100 is roughly estimated to deliver about. 3.958 10¹⁵ FLOPs (per second or per some standardised interval   here used as a comparative metric). Ideal (Perfect Efficiency) GPU hours. (Dividing total required FLOPs by per GPU capability gives) 3.3 10²⁴   3.958 10¹⁵    8.33 10⁸ seconds  0.4 million GPU hour Note: This \"perfect efficiency\" scenario is a lower bound. Real-world training is less efficient. 2. Adjusting for Real World Inefficiencies (Comparison with Llama 3.1) Reference Model: Llama 3.1 (405B parameters, 15 T tokens) reportedly required 30.84 M GPU hours in practice. Recalculating FLOPs for Llama 3.1: Using the same math: 3.64 10²⁵ FLOPs required Scaling Efficiency Using the ratio of FLOPs needed for DeepSeek V3 versus Llama 3.1. and assuming similar inefficiencies. The estimate adjusts to roughly 2.79M GPU hours for DeepSeek V3 training. 3. DeepSeek V3 Reported Training Breakdown According to the DeepSeek V3 paper Pre training Stage: Per Trillion Tokens: 180K H800 GPU hours Overall Pre training: Total of 2,664K GPU hours This stage was completed in less than two months using a cluster of 2,048 H800 GPUs. Context Length Extension: Additional 119K GPU hours Post training: An extra 5K GPU hours Total GPU Hours: 2,664 K 119 K 5 K 2.788M GPU hours Cost Estimation Assumed GPU Rental Price: $2 per GPU hour Total Rental Cost: 2.788M GPU hours $2 hour $5.576 million as stated in Deepseek paper During the pre training stage, training DeepSeek V3 on each trillion tokens requires only 180K H800 GPU hours  Consequently, our pre training stage is completed in less than two months and costs 2664K GPU hours. Combined with 119K GPU hours for the context length extension and 5K GPU hours for post training, DeepSeek V3 costs only 2.788M GPU hours for its full training. Assuming the rental price of the H800 GPU is $2 per GPU hour, our total training costs amount to only $5.576M. Summary Theoretical (Perfect Efficiency) Estimate:  0.4 M GPU hours (using idealised FLOP counts and assuming perfect hardware utilisation0 Adjusted (Real World) Estimate (via Llama 3.1 comparison):  2.79 GPU hours DeepSeek V3 Reported Breakdown: Pre training: 2,664K GPU hours Context Extension: 119K GPU hours Post training: 5K GPU hours Total:  2.788 M GPU hours Cost (at $2 per GPU hour):  $5.576 million\\n\\nBased on this information, please answer: How does DeepSeek V3 perform at reasoning and math compared to other models?', 'role': 'user'}, {'content': 'According to the document, DeepSeek V3 excels at reasoning and math tasks, surpassing the performance of GPT-4 and Claude 3.5 Sonnet in these areas. For writing and coding tasks, however, Claude 3.5 Sonnet maintains a slight lead over DeepSeek V3.', 'role': 'assistant'}], [{'content': 'Here is information about DeepSeek V3:\\n\\nBreakdown of the costs of the Deepseek v3 model Deepseek\\'s flagship model v3 showcases an architecture with a 671B parameter MOE (Mixture of Agents) with 37B active parameters per token Their success stems from breakthrough engineering: using MoE architecture, implementing FP8 mixed precision training, and developing a custom HAI-LLM framework. Deepseek excels at reasoning and math, surpassing GPT-4 and Claude 3.5 Sonnet. For writing and coding tasks, Claude 3.5 Sonnet maintains a slight lead. Deepseek pre-trained this model on 14.8 trillion high-quality data, taking 2,788,000 GPU hours on the Nvidia h800s cluster, costing around only $6 million the Llama 403b was trained on 11x of that, taking 30,840,000 GPU hours, also on 15 trillion tokens. So how true is the claim of $5.5 million, or is it another marketing trick? Underlying FLOP calculations Model Details: Active Parameters: 37B (using FP8 precision) FLOPs per token: Using the rule of thumb \"6 FLOPs per parameter per token.\" 37B 6   222B FLOPs per token Total Training Tokens: Approximately 14.8 trillion tokens Total FLOPs required: 222 B FLOPs token 14.8 T tokens   3.3 10²⁴ FLOPs GPU FLOP Capacity (H800 H100): An H100 is roughly estimated to deliver about. 3.958 10¹⁵ FLOPs (per second or per some standardised interval   here used as a comparative metric). Ideal (Perfect Efficiency) GPU hours. (Dividing total required FLOPs by per GPU capability gives) 3.3 10²⁴   3.958 10¹⁵    8.33 10⁸ seconds  0.4 million GPU hour Note: This \"perfect efficiency\" scenario is a lower bound. Real-world training is less efficient. 2. Adjusting for Real World Inefficiencies (Comparison with Llama 3.1) Reference Model: Llama 3.1 (405B parameters, 15 T tokens) reportedly required 30.84 M GPU hours in practice. Recalculating FLOPs for Llama 3.1: Using the same math: 3.64 10²⁵ FLOPs required Scaling Efficiency Using the ratio of FLOPs needed for DeepSeek V3 versus Llama 3.1. and assuming similar inefficiencies. The estimate adjusts to roughly 2.79M GPU hours for DeepSeek V3 training. 3. DeepSeek V3 Reported Training Breakdown According to the DeepSeek V3 paper Pre training Stage: Per Trillion Tokens: 180K H800 GPU hours Overall Pre training: Total of 2,664K GPU hours This stage was completed in less than two months using a cluster of 2,048 H800 GPUs. Context Length Extension: Additional 119K GPU hours Post training: An extra 5K GPU hours Total GPU Hours: 2,664 K 119 K 5 K 2.788M GPU hours Cost Estimation Assumed GPU Rental Price: $2 per GPU hour Total Rental Cost: 2.788M GPU hours $2 hour $5.576 million as stated in Deepseek paper During the pre training stage, training DeepSeek V3 on each trillion tokens requires only 180K H800 GPU hours  Consequently, our pre training stage is completed in less than two months and costs 2664K GPU hours. Combined with 119K GPU hours for the context length extension and 5K GPU hours for post training, DeepSeek V3 costs only 2.788M GPU hours for its full training. Assuming the rental price of the H800 GPU is $2 per GPU hour, our total training costs amount to only $5.576M. Summary Theoretical (Perfect Efficiency) Estimate:  0.4 M GPU hours (using idealised FLOP counts and assuming perfect hardware utilisation0 Adjusted (Real World) Estimate (via Llama 3.1 comparison):  2.79 GPU hours DeepSeek V3 Reported Breakdown: Pre training: 2,664K GPU hours Context Extension: 119K GPU hours Post training: 5K GPU hours Total:  2.788 M GPU hours Cost (at $2 per GPU hour):  $5.576 million\\n\\nBased on this information, please answer: What was the approximate cost of training DeepSeek V3 and how does it compare to other models?', 'role': 'user'}, {'content': \"The approximate cost of training DeepSeek V3 was around $5.5-5.6 million, which included approximately 2.788 million GPU hours at an estimated rate of $2 per GPU hour. The training used a cluster of 2,048 H800 GPUs and was completed in less than two months. This is significantly more efficient than competitor models like Llama 3.1, which reportedly required 30.84 million GPU hours for training on a similar amount of data (15 trillion tokens vs. DeepSeek's 14.8 trillion tokens).\", 'role': 'assistant'}], [{'content': \"Based on the research about DeepSeek V3, please answer the following question: What potential impact could DeepSeek V3's efficient approach have on the AI industry?\", 'role': 'user'}, {'content': \"DeepSeek V3's efficient approach could have several significant impacts on the AI industry: 1) It could democratize access to powerful AI by reducing the hardware requirements and costs, 2) It could accelerate AI development by enabling faster training cycles, 3) It could shift industry focus from hardware scaling to algorithmic efficiency, 4) It might lead to more environmentally sustainable AI by reducing energy consumption, and 5) It could disrupt the market dominance of major US tech companies, as evidenced by the market concerns and impact on share prices mentioned in the document.\", 'role': 'assistant'}], [{'content': 'Based on the DeepSeek V3 research paper, Could you elaborate on what is the hai-llm framework mentioned in the deepseek v3 paper?', 'role': 'user'}, {'content': 'HAI-LLM is a custom training framework developed by DeepSeek for training their V3 model. It includes several optimizations such as the DualPipe algorithm for efficient pipeline parallelism (which reduces pipeline bubbles and overlaps computation and communication), efficient cross-node all-to-all communication kernels to fully utilize network bandwidth, and careful memory optimizations to avoid using costly tensor parallelism.', 'role': 'assistant'}], [{'content': \"Based on the DeepSeek V3 research paper, What does the research say about what are the key differences between deepseek's approach and that of other ai companies?\", 'role': 'user'}, {'content': \"The key difference in DeepSeek's approach compared to other AI companies is that while others pushed for better hardware, DeepSeek focused on improving algorithms, achieving better results at a software level. Specifically, they made training 45 times more efficient by: using 8-bit instead of 32-bit to save memory, compressing key value indices with 93% compression ratios, implementing multi-token prediction instead of single-token prediction (doubling inference speeds), and using a Mixture-of-Experts model that decomposes a big model into small models capable of running on consumer-grade hardware.\", 'role': 'assistant'}], [{'content': \"Here is information about DeepSeek V3:\\n\\nauthor - Visith Kumarapperuma Deepseek V3: A Game-Changer in A.I. Here's Why It Matters Currently, the AI models from the Chinese startup Deepseek are causing quite a stir in the AI space. Their latest reasoning model, Deepseek r1, shows better or equal performance to competitors. But above all, they achieved it with a fraction of the training and inference cost. DeepSeek's AI Assistant overtook ChatGPT to become the most downloaded free app on the U.S. App Store. This development has led to market concerns about A.I. investments to major U.S. tech companies. Impacting share prices of tech firms including Nvidia.\\n\\nBased on this information, please answer: What potential impact could DeepSeek V3's efficient approach have on the AI industry?\", 'role': 'user'}, {'content': \"DeepSeek V3's efficient approach could have several significant impacts on the AI industry: 1) It could democratize access to powerful AI by reducing the hardware requirements and costs, 2) It could accelerate AI development by enabling faster training cycles, 3) It could shift industry focus from hardware scaling to algorithmic efficiency, 4) It might lead to more environmentally sustainable AI by reducing energy consumption, and 5) It could disrupt the market dominance of major US tech companies, as evidenced by the market concerns and impact on share prices mentioned in the document.\", 'role': 'assistant'}], [{'content': 'Based on the research about DeepSeek V3, please answer the following question: How many tokens was DeepSeek V3 trained on?', 'role': 'user'}, {'content': \"DeepSeek V3 was trained on approximately 14.8 trillion high-quality tokens. For perspective, this is comparable to Llama 3.1's 15 trillion tokens, but DeepSeek achieved much greater efficiency in its training process.\", 'role': 'assistant'}], [{'content': \"Here is information about DeepSeek V3:\\n\\nauthor - Visith Kumarapperuma Deepseek V3: A Game-Changer in A.I. Here's Why It Matters Currently, the AI models from the Chinese startup Deepseek are causing quite a stir in the AI space. Their latest reasoning model, Deepseek r1, shows better or equal performance to competitors. But above all, they achieved it with a fraction of the training and inference cost. DeepSeek's AI Assistant overtook ChatGPT to become the most downloaded free app on the U.S. App Store. This development has led to market concerns about A.I. investments to major U.S. tech companies. Impacting share prices of tech firms including Nvidia.\\n\\nBased on this information, please answer: What is DeepSeek V3 and why is it significant in the AI industry?\", 'role': 'user'}, {'content': 'DeepSeek V3 is an AI model from the Chinese startup Deepseek that has caused a stir in the AI space due to its impressive performance at a fraction of the training and inference cost compared to competitors. It uses a Mixture-of-Experts (MoE) architecture with 671B parameters but only 37B active parameters per token, which significantly reduces compute requirements. Its significance lies in improving algorithms at the software level rather than pushing for better hardware, making AI development more efficient and potentially disrupting the industry.', 'role': 'assistant'}], [{'content': 'Based on the research about DeepSeek V3, please answer the following question: What is the HAI-LLM framework mentioned in the DeepSeek V3 paper?', 'role': 'user'}, {'content': 'HAI-LLM is a custom training framework developed by DeepSeek for training their V3 model. It includes several optimizations such as the DualPipe algorithm for efficient pipeline parallelism (which reduces pipeline bubbles and overlaps computation and communication), efficient cross-node all-to-all communication kernels to fully utilize network bandwidth, and careful memory optimizations to avoid using costly tensor parallelism.', 'role': 'assistant'}], [{'content': 'Based on the DeepSeek V3 research paper, Could you elaborate on what was the approximate cost of training deepseek v3 and how does it compare to other models?', 'role': 'user'}, {'content': \"The approximate cost of training DeepSeek V3 was around $5.5-5.6 million, which included approximately 2.788 million GPU hours at an estimated rate of $2 per GPU hour. The training used a cluster of 2,048 H800 GPUs and was completed in less than two months. This is significantly more efficient than competitor models like Llama 3.1, which reportedly required 30.84 million GPU hours for training on a similar amount of data (15 trillion tokens vs. DeepSeek's 14.8 trillion tokens).\", 'role': 'assistant'}], [{'content': 'Here is information about DeepSeek V3:\\n\\nSo what made Deepseek such a big impact to A.I. ? The significance of Deepseek as a disruptor in the industry lies in its approach. Unlike other companies that pushed for better hardware, Deepseek improved the algorithms. Thus achieving better results at a software level. Note that the following details are for the Deepseek V3 model.   Deepseek said it trained a model using a data centre of some 2,000 of Nvidia H800 GPUs.   Time duration 2 months with the cost of the  final training run being  $5.5 million This  $5.5M reflects the \"rental\" cost for the GPU hours needed to train DeepSeek V3. It does not include: The capital expenditure for owning the hardware. Costs associated with prior research, ablation studies, or experiments on alternative architectures algorithms data. Deepseek made training more efficient (45 times more efficient) Use 8-bit instead of 32-bit to save memory. Compress key value indices which eat up a lot of VRAM; they got 93% compression ratios. Do multi-token prediction instead of single-token prediction -  doubled inference speeds The MOE model decomposes a big model into small models that can run on consumer-grade hardware.\\n\\nBased on this information, please answer: How did DeepSeek V3 achieve training efficiency compared to other large language models?', 'role': 'user'}, {'content': 'DeepSeek V3 achieved significant training efficiency through several innovations: 1) Using a Mixture-of-Experts (MoE) architecture where only 37B parameters fire for each token out of the total 671B, 2) Implementing FP8 mixed precision training which reduced memory usage by up to 50% compared to traditional formats, 3) Developing a custom training framework called HAI-LLM with optimizations like the DualPipe algorithm for efficient pipeline parallelism, 4) Using Multi-head Latent Attention (MLA) to compress the Key-Value cache, 5) Pioneering an auxiliary loss-free strategy for load balancing in the MoE architecture, and 6) Implementing efficient cross-node communication kernels and memory optimizations.', 'role': 'assistant'}], [{'content': 'Based on the DeepSeek V3 research paper, What does the research say about what is the hai-llm framework mentioned in the deepseek v3 paper?', 'role': 'user'}, {'content': 'HAI-LLM is a custom training framework developed by DeepSeek for training their V3 model. It includes several optimizations such as the DualPipe algorithm for efficient pipeline parallelism (which reduces pipeline bubbles and overlaps computation and communication), efficient cross-node all-to-all communication kernels to fully utilize network bandwidth, and careful memory optimizations to avoid using costly tensor parallelism.', 'role': 'assistant'}], [{'content': 'Based on the research about DeepSeek V3, please answer the following question: How does DeepSeek V3 perform at reasoning and math compared to other models?', 'role': 'user'}, {'content': 'According to the document, DeepSeek V3 excels at reasoning and math tasks, surpassing the performance of GPT-4 and Claude 3.5 Sonnet in these areas. For writing and coding tasks, however, Claude 3.5 Sonnet maintains a slight lead over DeepSeek V3.', 'role': 'assistant'}], [{'content': 'Here is information about DeepSeek V3:\\n\\nSo what made Deepseek such a big impact to A.I. ? The significance of Deepseek as a disruptor in the industry lies in its approach. Unlike other companies that pushed for better hardware, Deepseek improved the algorithms. Thus achieving better results at a software level. Note that the following details are for the Deepseek V3 model.   Deepseek said it trained a model using a data centre of some 2,000 of Nvidia H800 GPUs.   Time duration 2 months with the cost of the  final training run being  $5.5 million This  $5.5M reflects the \"rental\" cost for the GPU hours needed to train DeepSeek V3. It does not include: The capital expenditure for owning the hardware. Costs associated with prior research, ablation studies, or experiments on alternative architectures algorithms data. Deepseek made training more efficient (45 times more efficient) Use 8-bit instead of 32-bit to save memory. Compress key value indices which eat up a lot of VRAM; they got 93% compression ratios. Do multi-token prediction instead of single-token prediction -  doubled inference speeds The MOE model decomposes a big model into small models that can run on consumer-grade hardware.\\n\\nBased on this information, please answer: What compression ratio did DeepSeek achieve for key value indices?', 'role': 'user'}, {'content': 'According to the document, DeepSeek compressed key value indices which consume significant VRAM, achieving 93% compression ratios. This was part of their overall approach to make training more efficient.', 'role': 'assistant'}], [{'content': 'Based on the research about DeepSeek V3, please answer the following question: What was the approximate cost of training DeepSeek V3 and how does it compare to other models?', 'role': 'user'}, {'content': \"The approximate cost of training DeepSeek V3 was around $5.5-5.6 million, which included approximately 2.788 million GPU hours at an estimated rate of $2 per GPU hour. The training used a cluster of 2,048 H800 GPUs and was completed in less than two months. This is significantly more efficient than competitor models like Llama 3.1, which reportedly required 30.84 million GPU hours for training on a similar amount of data (15 trillion tokens vs. DeepSeek's 14.8 trillion tokens).\", 'role': 'assistant'}]] <class 'list'>\n",
            "['<|im_start|>user\\nBased on the DeepSeek V3 research paper, Could you elaborate on what is deepseek v3 and why is it significant in the ai industry?<|im_end|>\\n<|im_start|>assistant\\nDeepSeek V3 is an AI model from the Chinese startup Deepseek that has caused a stir in the AI space due to its impressive performance at a fraction of the training and inference cost compared to competitors. It uses a Mixture-of-Experts (MoE) architecture with 671B parameters but only 37B active parameters per token, which significantly reduces compute requirements. Its significance lies in improving algorithms at the software level rather than pushing for better hardware, making AI development more efficient and potentially disrupting the industry.<|im_end|>\\n', \"<|im_start|>user\\nBased on the research about DeepSeek V3, please answer the following question: What are the key differences between DeepSeek's approach and that of other AI companies?<|im_end|>\\n<|im_start|>assistant\\nThe key difference in DeepSeek's approach compared to other AI companies is that while others pushed for better hardware, DeepSeek focused on improving algorithms, achieving better results at a software level. Specifically, they made training 45 times more efficient by: using 8-bit instead of 32-bit to save memory, compressing key value indices with 93% compression ratios, implementing multi-token prediction instead of single-token prediction (doubling inference speeds), and using a Mixture-of-Experts model that decomposes a big model into small models capable of running on consumer-grade hardware.<|im_end|>\\n\", '<|im_start|>user\\nBased on the research about DeepSeek V3, please answer the following question: How many GPU hours did it take to train DeepSeek V3?<|im_end|>\\n<|im_start|>assistant\\nTraining DeepSeek V3 took approximately 2.788 million GPU hours in total. This breaks down to 2,664,000 GPU hours for pre-training, 119,000 GPU hours for context length extension, and 5,000 GPU hours for post-training. The training was completed in less than two months using a cluster of 2,048 H800 GPUs.<|im_end|>\\n', '<|im_start|>user\\nBased on the DeepSeek V3 research paper, What does the research say about what type of architecture does deepseek v3 use?<|im_end|>\\n<|im_start|>assistant\\nDeepSeek V3 uses a Mixture-of-Experts (MoE) architecture where only 37B parameters fire for each token out of the total 671B parameters. This sparse activation significantly reduces compute requirements compared to dense models. The model also uses Multi-head Latent Attention (MLA) which compresses the Key-Value cache, reducing memory usage and enabling more efficient training.<|im_end|>\\n', \"<|im_start|>user\\nHere is information about DeepSeek V3:\\n\\nFP8 Mixed Precision Training: They implemented an FP8 mixed precision training framework. Which reduces memory usage and accelerates training compared to higher precision formats. Reduced memory footprint by up to 50% compared to traditional FP16 FP32 formats. They use fine-grained quantisation strategies and increased accumulation precision to maintain accuracy.\\n\\nBased on this information, please answer: What was DeepSeek's approach to load balancing in their MoE architecture?<|im_end|>\\n<|im_start|>assistant\\nDeepSeek pioneered an auxiliary loss-free strategy for load balancing in their Mixture-of-Experts (MoE) architecture. This improved performance without the drawbacks of traditional auxiliary loss methods that are typically used in MoE architectures.<|im_end|>\\n\", '<|im_start|>user\\nBased on the DeepSeek V3 research paper, Please explain how does deepseek v3 perform at reasoning and math compared to other models.<|im_end|>\\n<|im_start|>assistant\\nAccording to the document, DeepSeek V3 excels at reasoning and math tasks, surpassing the performance of GPT-4 and Claude 3.5 Sonnet in these areas. For writing and coding tasks, however, Claude 3.5 Sonnet maintains a slight lead over DeepSeek V3.<|im_end|>\\n', '<|im_start|>user\\nBased on the DeepSeek V3 research paper, What information do you have on how many gpu hours did it take to train deepseek v3?<|im_end|>\\n<|im_start|>assistant\\nTraining DeepSeek V3 took approximately 2.788 million GPU hours in total. This breaks down to 2,664,000 GPU hours for pre-training, 119,000 GPU hours for context length extension, and 5,000 GPU hours for post-training. The training was completed in less than two months using a cluster of 2,048 H800 GPUs.<|im_end|>\\n', \"<|im_start|>user\\nBased on the DeepSeek V3 research paper, What does the research say about what was the approximate cost of training deepseek v3 and how does it compare to other models?<|im_end|>\\n<|im_start|>assistant\\nThe approximate cost of training DeepSeek V3 was around $5.5-5.6 million, which included approximately 2.788 million GPU hours at an estimated rate of $2 per GPU hour. The training used a cluster of 2,048 H800 GPUs and was completed in less than two months. This is significantly more efficient than competitor models like Llama 3.1, which reportedly required 30.84 million GPU hours for training on a similar amount of data (15 trillion tokens vs. DeepSeek's 14.8 trillion tokens).<|im_end|>\\n\", '<|im_start|>user\\nBased on the DeepSeek V3 research paper, What information do you have on what compression ratio did deepseek achieve for key value indices?<|im_end|>\\n<|im_start|>assistant\\nAccording to the document, DeepSeek compressed key value indices which consume significant VRAM, achieving 93% compression ratios. This was part of their overall approach to make training more efficient.<|im_end|>\\n', \"<|im_start|>user\\nBased on the DeepSeek V3 research paper, What information do you have on how many tokens was deepseek v3 trained on?<|im_end|>\\n<|im_start|>assistant\\nDeepSeek V3 was trained on approximately 14.8 trillion high-quality tokens. For perspective, this is comparable to Llama 3.1's 15 trillion tokens, but DeepSeek achieved much greater efficiency in its training process.<|im_end|>\\n\", '<|im_start|>user\\nBased on the DeepSeek V3 research paper, What information do you have on how many parameters does deepseek v3 have and how many are active during inference?<|im_end|>\\n<|im_start|>assistant\\nDeepSeek V3 has a total of 671B parameters in its Mixture-of-Experts (MoE) architecture, but only 37B parameters are active (fire) for each token during processing. This sparse activation approach significantly reduces the computational requirements compared to dense models of similar size.<|im_end|>\\n', '<|im_start|>user\\nBased on the DeepSeek V3 research paper, What information do you have on what is multi-head latent attention (mla) and how was it used in deepseek v3?<|im_end|>\\n<|im_start|>assistant\\nMulti-head Latent Attention (MLA) is a technique used in DeepSeek V3 that compresses the Key-Value cache. This reduces memory usage and enables more efficient training by decreasing the memory footprint required during model operation.<|im_end|>\\n', '<|im_start|>user\\nBased on the research about DeepSeek V3, please answer the following question: What is Multi-head Latent Attention (MLA) and how was it used in DeepSeek V3?<|im_end|>\\n<|im_start|>assistant\\nMulti-head Latent Attention (MLA) is a technique used in DeepSeek V3 that compresses the Key-Value cache. This reduces memory usage and enables more efficient training by decreasing the memory footprint required during model operation.<|im_end|>\\n', \"<|im_start|>user\\nBased on the DeepSeek V3 research paper, What information do you have on what was deepseek's approach to load balancing in their moe architecture?<|im_end|>\\n<|im_start|>assistant\\nDeepSeek pioneered an auxiliary loss-free strategy for load balancing in their Mixture-of-Experts (MoE) architecture. This improved performance without the drawbacks of traditional auxiliary loss methods that are typically used in MoE architectures.<|im_end|>\\n\", '<|im_start|>user\\nBased on the DeepSeek V3 research paper, What information do you have on what type of architecture does deepseek v3 use?<|im_end|>\\n<|im_start|>assistant\\nDeepSeek V3 uses a Mixture-of-Experts (MoE) architecture where only 37B parameters fire for each token out of the total 671B parameters. This sparse activation significantly reduces compute requirements compared to dense models. The model also uses Multi-head Latent Attention (MLA) which compresses the Key-Value cache, reducing memory usage and enabling more efficient training.<|im_end|>\\n', '<|im_start|>user\\nBased on the research about DeepSeek V3, please answer the following question: What type of architecture does DeepSeek V3 use?<|im_end|>\\n<|im_start|>assistant\\nDeepSeek V3 uses a Mixture-of-Experts (MoE) architecture where only 37B parameters fire for each token out of the total 671B parameters. This sparse activation significantly reduces compute requirements compared to dense models. The model also uses Multi-head Latent Attention (MLA) which compresses the Key-Value cache, reducing memory usage and enabling more efficient training.<|im_end|>\\n', '<|im_start|>user\\nBased on the DeepSeek V3 research paper, What information do you have on how might the techniques used in deepseek v3 be applied to smaller models or different domains?<|im_end|>\\n<|im_start|>assistant\\nThe techniques used in DeepSeek V3 could be applied to smaller models or different domains in several ways: 1) The Mixture-of-Experts architecture could help create more efficient smaller models by activating only relevant parameters, 2) FP8 mixed precision training could reduce memory requirements for models of any size, 3) The Key-Value cache compression techniques could improve efficiency in models designed for limited hardware, 4) Multi-token prediction could speed up inference across various applications, and 5) The custom training framework optimizations could benefit any distributed training setup regardless of model size or application domain.<|im_end|>\\n', '<|im_start|>user\\nBased on the DeepSeek V3 research paper, Please explain what is deepseek v3 and why is it significant in the ai industry.<|im_end|>\\n<|im_start|>assistant\\nDeepSeek V3 is an AI model from the Chinese startup Deepseek that has caused a stir in the AI space due to its impressive performance at a fraction of the training and inference cost compared to competitors. It uses a Mixture-of-Experts (MoE) architecture with 671B parameters but only 37B active parameters per token, which significantly reduces compute requirements. Its significance lies in improving algorithms at the software level rather than pushing for better hardware, making AI development more efficient and potentially disrupting the industry.<|im_end|>\\n', '<|im_start|>user\\nHere is information about DeepSeek V3:\\n\\nBreakdown of the costs of the Deepseek v3 model Deepseek\\'s flagship model v3 showcases an architecture with a 671B parameter MOE (Mixture of Agents) with 37B active parameters per token Their success stems from breakthrough engineering: using MoE architecture, implementing FP8 mixed precision training, and developing a custom HAI-LLM framework. Deepseek excels at reasoning and math, surpassing GPT-4 and Claude 3.5 Sonnet. For writing and coding tasks, Claude 3.5 Sonnet maintains a slight lead. Deepseek pre-trained this model on 14.8 trillion high-quality data, taking 2,788,000 GPU hours on the Nvidia h800s cluster, costing around only $6 million the Llama 403b was trained on 11x of that, taking 30,840,000 GPU hours, also on 15 trillion tokens. So how true is the claim of $5.5 million, or is it another marketing trick? Underlying FLOP calculations Model Details: Active Parameters: 37B (using FP8 precision) FLOPs per token: Using the rule of thumb \"6 FLOPs per parameter per token.\" 37B 6   222B FLOPs per token Total Training Tokens: Approximately 14.8 trillion tokens Total FLOPs required: 222 B FLOPs token 14.8 T tokens   3.3 10²⁴ FLOPs GPU FLOP Capacity (H800 H100): An H100 is roughly estimated to deliver about. 3.958 10¹⁵ FLOPs (per second or per some standardised interval   here used as a comparative metric). Ideal (Perfect Efficiency) GPU hours. (Dividing total required FLOPs by per GPU capability gives) 3.3 10²⁴   3.958 10¹⁵    8.33 10⁸ seconds  0.4 million GPU hour Note: This \"perfect efficiency\" scenario is a lower bound. Real-world training is less efficient. 2. Adjusting for Real World Inefficiencies (Comparison with Llama 3.1) Reference Model: Llama 3.1 (405B parameters, 15 T tokens) reportedly required 30.84 M GPU hours in practice. Recalculating FLOPs for Llama 3.1: Using the same math: 3.64 10²⁵ FLOPs required Scaling Efficiency Using the ratio of FLOPs needed for DeepSeek V3 versus Llama 3.1. and assuming similar inefficiencies. The estimate adjusts to roughly 2.79M GPU hours for DeepSeek V3 training. 3. DeepSeek V3 Reported Training Breakdown According to the DeepSeek V3 paper Pre training Stage: Per Trillion Tokens: 180K H800 GPU hours Overall Pre training: Total of 2,664K GPU hours This stage was completed in less than two months using a cluster of 2,048 H800 GPUs. Context Length Extension: Additional 119K GPU hours Post training: An extra 5K GPU hours Total GPU Hours: 2,664 K 119 K 5 K 2.788M GPU hours Cost Estimation Assumed GPU Rental Price: $2 per GPU hour Total Rental Cost: 2.788M GPU hours $2 hour $5.576 million as stated in Deepseek paper During the pre training stage, training DeepSeek V3 on each trillion tokens requires only 180K H800 GPU hours  Consequently, our pre training stage is completed in less than two months and costs 2664K GPU hours. Combined with 119K GPU hours for the context length extension and 5K GPU hours for post training, DeepSeek V3 costs only 2.788M GPU hours for its full training. Assuming the rental price of the H800 GPU is $2 per GPU hour, our total training costs amount to only $5.576M. Summary Theoretical (Perfect Efficiency) Estimate:  0.4 M GPU hours (using idealised FLOP counts and assuming perfect hardware utilisation0 Adjusted (Real World) Estimate (via Llama 3.1 comparison):  2.79 GPU hours DeepSeek V3 Reported Breakdown: Pre training: 2,664K GPU hours Context Extension: 119K GPU hours Post training: 5K GPU hours Total:  2.788 M GPU hours Cost (at $2 per GPU hour):  $5.576 million\\n\\nBased on this information, please answer: How does the training efficiency of DeepSeek V3 compare to Llama 3.1?<|im_end|>\\n<|im_start|>assistant\\nDeepSeek V3 was significantly more efficient in training compared to Llama 3.1. While DeepSeek V3 required about 2.788 million GPU hours to train on 14.8 trillion tokens, Llama 3.1 reportedly required 30.84 million GPU hours to train on a similar amount of data (15 trillion tokens). This means DeepSeek V3 was approximately 11 times more efficient than Llama 3.1 in terms of GPU hours required for training.<|im_end|>\\n', '<|im_start|>user\\nBased on the research about DeepSeek V3, please answer the following question: How did DeepSeek V3 achieve training efficiency compared to other large language models?<|im_end|>\\n<|im_start|>assistant\\nDeepSeek V3 achieved significant training efficiency through several innovations: 1) Using a Mixture-of-Experts (MoE) architecture where only 37B parameters fire for each token out of the total 671B, 2) Implementing FP8 mixed precision training which reduced memory usage by up to 50% compared to traditional formats, 3) Developing a custom training framework called HAI-LLM with optimizations like the DualPipe algorithm for efficient pipeline parallelism, 4) Using Multi-head Latent Attention (MLA) to compress the Key-Value cache, 5) Pioneering an auxiliary loss-free strategy for load balancing in the MoE architecture, and 6) Implementing efficient cross-node communication kernels and memory optimizations.<|im_end|>\\n', '<|im_start|>user\\nBased on the DeepSeek V3 research paper, Please explain what compression ratio did deepseek achieve for key value indices.<|im_end|>\\n<|im_start|>assistant\\nAccording to the document, DeepSeek compressed key value indices which consume significant VRAM, achieving 93% compression ratios. This was part of their overall approach to make training more efficient.<|im_end|>\\n', '<|im_start|>user\\nBased on the DeepSeek V3 research paper, Could you elaborate on how does the training efficiency of deepseek v3 compare to llama 3.1?<|im_end|>\\n<|im_start|>assistant\\nDeepSeek V3 was significantly more efficient in training compared to Llama 3.1. While DeepSeek V3 required about 2.788 million GPU hours to train on 14.8 trillion tokens, Llama 3.1 reportedly required 30.84 million GPU hours to train on a similar amount of data (15 trillion tokens). This means DeepSeek V3 was approximately 11 times more efficient than Llama 3.1 in terms of GPU hours required for training.<|im_end|>\\n', '<|im_start|>user\\nHere is information about DeepSeek V3:\\n\\nModel Architecture The model employs a Mixture-of-Experts (MoE) architecture, where only 37B parameters fire for each token out of the total 671B. This sparse activation significantly reduces compute requirements compared to dense models. The model uses Multi-head Latent Attention (MLA). This compresses the Key-Value cache, reducing memory usage and enabling more efficient training.\\n\\nBased on this information, please answer: What type of architecture does DeepSeek V3 use?<|im_end|>\\n<|im_start|>assistant\\nDeepSeek V3 uses a Mixture-of-Experts (MoE) architecture where only 37B parameters fire for each token out of the total 671B parameters. This sparse activation significantly reduces compute requirements compared to dense models. The model also uses Multi-head Latent Attention (MLA) which compresses the Key-Value cache, reducing memory usage and enabling more efficient training.<|im_end|>\\n', '<|im_start|>user\\nBased on the DeepSeek V3 research paper, Can you tell me about how did deepseek v3 achieve training efficiency compared to other large language models?<|im_end|>\\n<|im_start|>assistant\\nDeepSeek V3 achieved significant training efficiency through several innovations: 1) Using a Mixture-of-Experts (MoE) architecture where only 37B parameters fire for each token out of the total 671B, 2) Implementing FP8 mixed precision training which reduced memory usage by up to 50% compared to traditional formats, 3) Developing a custom training framework called HAI-LLM with optimizations like the DualPipe algorithm for efficient pipeline parallelism, 4) Using Multi-head Latent Attention (MLA) to compress the Key-Value cache, 5) Pioneering an auxiliary loss-free strategy for load balancing in the MoE architecture, and 6) Implementing efficient cross-node communication kernels and memory optimizations.<|im_end|>\\n', '<|im_start|>user\\nHere is information about DeepSeek V3:\\n\\nModel Architecture The model employs a Mixture-of-Experts (MoE) architecture, where only 37B parameters fire for each token out of the total 671B. This sparse activation significantly reduces compute requirements compared to dense models. The model uses Multi-head Latent Attention (MLA). This compresses the Key-Value cache, reducing memory usage and enabling more efficient training.\\n\\nBased on this information, please answer: How might the techniques used in DeepSeek V3 be applied to smaller models or different domains?<|im_end|>\\n<|im_start|>assistant\\nThe techniques used in DeepSeek V3 could be applied to smaller models or different domains in several ways: 1) The Mixture-of-Experts architecture could help create more efficient smaller models by activating only relevant parameters, 2) FP8 mixed precision training could reduce memory requirements for models of any size, 3) The Key-Value cache compression techniques could improve efficiency in models designed for limited hardware, 4) Multi-token prediction could speed up inference across various applications, and 5) The custom training framework optimizations could benefit any distributed training setup regardless of model size or application domain.<|im_end|>\\n', '<|im_start|>user\\nBased on the DeepSeek V3 research paper, Could you elaborate on how might the techniques used in deepseek v3 be applied to smaller models or different domains?<|im_end|>\\n<|im_start|>assistant\\nThe techniques used in DeepSeek V3 could be applied to smaller models or different domains in several ways: 1) The Mixture-of-Experts architecture could help create more efficient smaller models by activating only relevant parameters, 2) FP8 mixed precision training could reduce memory requirements for models of any size, 3) The Key-Value cache compression techniques could improve efficiency in models designed for limited hardware, 4) Multi-token prediction could speed up inference across various applications, and 5) The custom training framework optimizations could benefit any distributed training setup regardless of model size or application domain.<|im_end|>\\n', \"<|im_start|>user\\nBased on the DeepSeek V3 research paper, I'd like to know more about what precision format did deepseek v3 use for training and what were its benefits.<|im_end|>\\n<|im_start|>assistant\\nDeepSeek V3 implemented an FP8 mixed precision training framework, which reduced memory usage and accelerated training compared to higher precision formats. This approach reduced the memory footprint by up to 50% compared to traditional FP16/FP32 formats. They used fine-grained quantization strategies and increased accumulation precision to maintain accuracy while gaining the efficiency benefits.<|im_end|>\\n\", \"<|im_start|>user\\nBased on the DeepSeek V3 research paper, Please explain what potential impact could deepseek v3's efficient approach have on the ai industry.<|im_end|>\\n<|im_start|>assistant\\nDeepSeek V3's efficient approach could have several significant impacts on the AI industry: 1) It could democratize access to powerful AI by reducing the hardware requirements and costs, 2) It could accelerate AI development by enabling faster training cycles, 3) It could shift industry focus from hardware scaling to algorithmic efficiency, 4) It might lead to more environmentally sustainable AI by reducing energy consumption, and 5) It could disrupt the market dominance of major US tech companies, as evidenced by the market concerns and impact on share prices mentioned in the document.<|im_end|>\\n\", '<|im_start|>user\\nBased on the DeepSeek V3 research paper, What information do you have on what precision format did deepseek v3 use for training and what were its benefits?<|im_end|>\\n<|im_start|>assistant\\nDeepSeek V3 implemented an FP8 mixed precision training framework, which reduced memory usage and accelerated training compared to higher precision formats. This approach reduced the memory footprint by up to 50% compared to traditional FP16/FP32 formats. They used fine-grained quantization strategies and increased accumulation precision to maintain accuracy while gaining the efficiency benefits.<|im_end|>\\n', '<|im_start|>user\\nHere is information about DeepSeek V3:\\n\\nModel Architecture The model employs a Mixture-of-Experts (MoE) architecture, where only 37B parameters fire for each token out of the total 671B. This sparse activation significantly reduces compute requirements compared to dense models. The model uses Multi-head Latent Attention (MLA). This compresses the Key-Value cache, reducing memory usage and enabling more efficient training.\\n\\nBased on this information, please answer: How many parameters does DeepSeek V3 have and how many are active during inference?<|im_end|>\\n<|im_start|>assistant\\nDeepSeek V3 has a total of 671B parameters in its Mixture-of-Experts (MoE) architecture, but only 37B parameters are active (fire) for each token during processing. This sparse activation approach significantly reduces the computational requirements compared to dense models of similar size.<|im_end|>\\n', '<|im_start|>user\\nHere is information about DeepSeek V3:\\n\\nFP8 Mixed Precision Training: They implemented an FP8 mixed precision training framework. Which reduces memory usage and accelerates training compared to higher precision formats. Reduced memory footprint by up to 50% compared to traditional FP16 FP32 formats. They use fine-grained quantisation strategies and increased accumulation precision to maintain accuracy.\\n\\nBased on this information, please answer: What precision format did DeepSeek V3 use for training and what were its benefits?<|im_end|>\\n<|im_start|>assistant\\nDeepSeek V3 implemented an FP8 mixed precision training framework, which reduced memory usage and accelerated training compared to higher precision formats. This approach reduced the memory footprint by up to 50% compared to traditional FP16/FP32 formats. They used fine-grained quantization strategies and increased accumulation precision to maintain accuracy while gaining the efficiency benefits.<|im_end|>\\n', \"<|im_start|>user\\nBased on the DeepSeek V3 research paper, Please explain what potential impact could deepseek v3's efficient approach have on the ai industry.<|im_end|>\\n<|im_start|>assistant\\nDeepSeek V3's efficient approach could have several significant impacts on the AI industry: 1) It could democratize access to powerful AI by reducing the hardware requirements and costs, 2) It could accelerate AI development by enabling faster training cycles, 3) It could shift industry focus from hardware scaling to algorithmic efficiency, 4) It might lead to more environmentally sustainable AI by reducing energy consumption, and 5) It could disrupt the market dominance of major US tech companies, as evidenced by the market concerns and impact on share prices mentioned in the document.<|im_end|>\\n\", '<|im_start|>user\\nBased on the DeepSeek V3 research paper, Could you elaborate on how did deepseek v3 achieve training efficiency compared to other large language models?<|im_end|>\\n<|im_start|>assistant\\nDeepSeek V3 achieved significant training efficiency through several innovations: 1) Using a Mixture-of-Experts (MoE) architecture where only 37B parameters fire for each token out of the total 671B, 2) Implementing FP8 mixed precision training which reduced memory usage by up to 50% compared to traditional formats, 3) Developing a custom training framework called HAI-LLM with optimizations like the DualPipe algorithm for efficient pipeline parallelism, 4) Using Multi-head Latent Attention (MLA) to compress the Key-Value cache, 5) Pioneering an auxiliary loss-free strategy for load balancing in the MoE architecture, and 6) Implementing efficient cross-node communication kernels and memory optimizations.<|im_end|>\\n', '<|im_start|>user\\nHere is information about DeepSeek V3:\\n\\nBreakdown of the costs of the Deepseek v3 model Deepseek\\'s flagship model v3 showcases an architecture with a 671B parameter MOE (Mixture of Agents) with 37B active parameters per token Their success stems from breakthrough engineering: using MoE architecture, implementing FP8 mixed precision training, and developing a custom HAI-LLM framework. Deepseek excels at reasoning and math, surpassing GPT-4 and Claude 3.5 Sonnet. For writing and coding tasks, Claude 3.5 Sonnet maintains a slight lead. Deepseek pre-trained this model on 14.8 trillion high-quality data, taking 2,788,000 GPU hours on the Nvidia h800s cluster, costing around only $6 million the Llama 403b was trained on 11x of that, taking 30,840,000 GPU hours, also on 15 trillion tokens. So how true is the claim of $5.5 million, or is it another marketing trick? Underlying FLOP calculations Model Details: Active Parameters: 37B (using FP8 precision) FLOPs per token: Using the rule of thumb \"6 FLOPs per parameter per token.\" 37B 6   222B FLOPs per token Total Training Tokens: Approximately 14.8 trillion tokens Total FLOPs required: 222 B FLOPs token 14.8 T tokens   3.3 10²⁴ FLOPs GPU FLOP Capacity (H800 H100): An H100 is roughly estimated to deliver about. 3.958 10¹⁵ FLOPs (per second or per some standardised interval   here used as a comparative metric). Ideal (Perfect Efficiency) GPU hours. (Dividing total required FLOPs by per GPU capability gives) 3.3 10²⁴   3.958 10¹⁵    8.33 10⁸ seconds  0.4 million GPU hour Note: This \"perfect efficiency\" scenario is a lower bound. Real-world training is less efficient. 2. Adjusting for Real World Inefficiencies (Comparison with Llama 3.1) Reference Model: Llama 3.1 (405B parameters, 15 T tokens) reportedly required 30.84 M GPU hours in practice. Recalculating FLOPs for Llama 3.1: Using the same math: 3.64 10²⁵ FLOPs required Scaling Efficiency Using the ratio of FLOPs needed for DeepSeek V3 versus Llama 3.1. and assuming similar inefficiencies. The estimate adjusts to roughly 2.79M GPU hours for DeepSeek V3 training. 3. DeepSeek V3 Reported Training Breakdown According to the DeepSeek V3 paper Pre training Stage: Per Trillion Tokens: 180K H800 GPU hours Overall Pre training: Total of 2,664K GPU hours This stage was completed in less than two months using a cluster of 2,048 H800 GPUs. Context Length Extension: Additional 119K GPU hours Post training: An extra 5K GPU hours Total GPU Hours: 2,664 K 119 K 5 K 2.788M GPU hours Cost Estimation Assumed GPU Rental Price: $2 per GPU hour Total Rental Cost: 2.788M GPU hours $2 hour $5.576 million as stated in Deepseek paper During the pre training stage, training DeepSeek V3 on each trillion tokens requires only 180K H800 GPU hours  Consequently, our pre training stage is completed in less than two months and costs 2664K GPU hours. Combined with 119K GPU hours for the context length extension and 5K GPU hours for post training, DeepSeek V3 costs only 2.788M GPU hours for its full training. Assuming the rental price of the H800 GPU is $2 per GPU hour, our total training costs amount to only $5.576M. Summary Theoretical (Perfect Efficiency) Estimate:  0.4 M GPU hours (using idealised FLOP counts and assuming perfect hardware utilisation0 Adjusted (Real World) Estimate (via Llama 3.1 comparison):  2.79 GPU hours DeepSeek V3 Reported Breakdown: Pre training: 2,664K GPU hours Context Extension: 119K GPU hours Post training: 5K GPU hours Total:  2.788 M GPU hours Cost (at $2 per GPU hour):  $5.576 million\\n\\nBased on this information, please answer: How does DeepSeek V3 perform at reasoning and math compared to other models?<|im_end|>\\n<|im_start|>assistant\\nAccording to the document, DeepSeek V3 excels at reasoning and math tasks, surpassing the performance of GPT-4 and Claude 3.5 Sonnet in these areas. For writing and coding tasks, however, Claude 3.5 Sonnet maintains a slight lead over DeepSeek V3.<|im_end|>\\n', '<|im_start|>user\\nHere is information about DeepSeek V3:\\n\\nBreakdown of the costs of the Deepseek v3 model Deepseek\\'s flagship model v3 showcases an architecture with a 671B parameter MOE (Mixture of Agents) with 37B active parameters per token Their success stems from breakthrough engineering: using MoE architecture, implementing FP8 mixed precision training, and developing a custom HAI-LLM framework. Deepseek excels at reasoning and math, surpassing GPT-4 and Claude 3.5 Sonnet. For writing and coding tasks, Claude 3.5 Sonnet maintains a slight lead. Deepseek pre-trained this model on 14.8 trillion high-quality data, taking 2,788,000 GPU hours on the Nvidia h800s cluster, costing around only $6 million the Llama 403b was trained on 11x of that, taking 30,840,000 GPU hours, also on 15 trillion tokens. So how true is the claim of $5.5 million, or is it another marketing trick? Underlying FLOP calculations Model Details: Active Parameters: 37B (using FP8 precision) FLOPs per token: Using the rule of thumb \"6 FLOPs per parameter per token.\" 37B 6   222B FLOPs per token Total Training Tokens: Approximately 14.8 trillion tokens Total FLOPs required: 222 B FLOPs token 14.8 T tokens   3.3 10²⁴ FLOPs GPU FLOP Capacity (H800 H100): An H100 is roughly estimated to deliver about. 3.958 10¹⁵ FLOPs (per second or per some standardised interval   here used as a comparative metric). Ideal (Perfect Efficiency) GPU hours. (Dividing total required FLOPs by per GPU capability gives) 3.3 10²⁴   3.958 10¹⁵    8.33 10⁸ seconds  0.4 million GPU hour Note: This \"perfect efficiency\" scenario is a lower bound. Real-world training is less efficient. 2. Adjusting for Real World Inefficiencies (Comparison with Llama 3.1) Reference Model: Llama 3.1 (405B parameters, 15 T tokens) reportedly required 30.84 M GPU hours in practice. Recalculating FLOPs for Llama 3.1: Using the same math: 3.64 10²⁵ FLOPs required Scaling Efficiency Using the ratio of FLOPs needed for DeepSeek V3 versus Llama 3.1. and assuming similar inefficiencies. The estimate adjusts to roughly 2.79M GPU hours for DeepSeek V3 training. 3. DeepSeek V3 Reported Training Breakdown According to the DeepSeek V3 paper Pre training Stage: Per Trillion Tokens: 180K H800 GPU hours Overall Pre training: Total of 2,664K GPU hours This stage was completed in less than two months using a cluster of 2,048 H800 GPUs. Context Length Extension: Additional 119K GPU hours Post training: An extra 5K GPU hours Total GPU Hours: 2,664 K 119 K 5 K 2.788M GPU hours Cost Estimation Assumed GPU Rental Price: $2 per GPU hour Total Rental Cost: 2.788M GPU hours $2 hour $5.576 million as stated in Deepseek paper During the pre training stage, training DeepSeek V3 on each trillion tokens requires only 180K H800 GPU hours  Consequently, our pre training stage is completed in less than two months and costs 2664K GPU hours. Combined with 119K GPU hours for the context length extension and 5K GPU hours for post training, DeepSeek V3 costs only 2.788M GPU hours for its full training. Assuming the rental price of the H800 GPU is $2 per GPU hour, our total training costs amount to only $5.576M. Summary Theoretical (Perfect Efficiency) Estimate:  0.4 M GPU hours (using idealised FLOP counts and assuming perfect hardware utilisation0 Adjusted (Real World) Estimate (via Llama 3.1 comparison):  2.79 GPU hours DeepSeek V3 Reported Breakdown: Pre training: 2,664K GPU hours Context Extension: 119K GPU hours Post training: 5K GPU hours Total:  2.788 M GPU hours Cost (at $2 per GPU hour):  $5.576 million\\n\\nBased on this information, please answer: What was the approximate cost of training DeepSeek V3 and how does it compare to other models?<|im_end|>\\n<|im_start|>assistant\\nThe approximate cost of training DeepSeek V3 was around $5.5-5.6 million, which included approximately 2.788 million GPU hours at an estimated rate of $2 per GPU hour. The training used a cluster of 2,048 H800 GPUs and was completed in less than two months. This is significantly more efficient than competitor models like Llama 3.1, which reportedly required 30.84 million GPU hours for training on a similar amount of data (15 trillion tokens vs. DeepSeek\\'s 14.8 trillion tokens).<|im_end|>\\n', \"<|im_start|>user\\nBased on the research about DeepSeek V3, please answer the following question: What potential impact could DeepSeek V3's efficient approach have on the AI industry?<|im_end|>\\n<|im_start|>assistant\\nDeepSeek V3's efficient approach could have several significant impacts on the AI industry: 1) It could democratize access to powerful AI by reducing the hardware requirements and costs, 2) It could accelerate AI development by enabling faster training cycles, 3) It could shift industry focus from hardware scaling to algorithmic efficiency, 4) It might lead to more environmentally sustainable AI by reducing energy consumption, and 5) It could disrupt the market dominance of major US tech companies, as evidenced by the market concerns and impact on share prices mentioned in the document.<|im_end|>\\n\", '<|im_start|>user\\nBased on the DeepSeek V3 research paper, Could you elaborate on what is the hai-llm framework mentioned in the deepseek v3 paper?<|im_end|>\\n<|im_start|>assistant\\nHAI-LLM is a custom training framework developed by DeepSeek for training their V3 model. It includes several optimizations such as the DualPipe algorithm for efficient pipeline parallelism (which reduces pipeline bubbles and overlaps computation and communication), efficient cross-node all-to-all communication kernels to fully utilize network bandwidth, and careful memory optimizations to avoid using costly tensor parallelism.<|im_end|>\\n', \"<|im_start|>user\\nBased on the DeepSeek V3 research paper, What does the research say about what are the key differences between deepseek's approach and that of other ai companies?<|im_end|>\\n<|im_start|>assistant\\nThe key difference in DeepSeek's approach compared to other AI companies is that while others pushed for better hardware, DeepSeek focused on improving algorithms, achieving better results at a software level. Specifically, they made training 45 times more efficient by: using 8-bit instead of 32-bit to save memory, compressing key value indices with 93% compression ratios, implementing multi-token prediction instead of single-token prediction (doubling inference speeds), and using a Mixture-of-Experts model that decomposes a big model into small models capable of running on consumer-grade hardware.<|im_end|>\\n\", \"<|im_start|>user\\nHere is information about DeepSeek V3:\\n\\nauthor - Visith Kumarapperuma Deepseek V3: A Game-Changer in A.I. Here's Why It Matters Currently, the AI models from the Chinese startup Deepseek are causing quite a stir in the AI space. Their latest reasoning model, Deepseek r1, shows better or equal performance to competitors. But above all, they achieved it with a fraction of the training and inference cost. DeepSeek's AI Assistant overtook ChatGPT to become the most downloaded free app on the U.S. App Store. This development has led to market concerns about A.I. investments to major U.S. tech companies. Impacting share prices of tech firms including Nvidia.\\n\\nBased on this information, please answer: What potential impact could DeepSeek V3's efficient approach have on the AI industry?<|im_end|>\\n<|im_start|>assistant\\nDeepSeek V3's efficient approach could have several significant impacts on the AI industry: 1) It could democratize access to powerful AI by reducing the hardware requirements and costs, 2) It could accelerate AI development by enabling faster training cycles, 3) It could shift industry focus from hardware scaling to algorithmic efficiency, 4) It might lead to more environmentally sustainable AI by reducing energy consumption, and 5) It could disrupt the market dominance of major US tech companies, as evidenced by the market concerns and impact on share prices mentioned in the document.<|im_end|>\\n\", \"<|im_start|>user\\nBased on the research about DeepSeek V3, please answer the following question: How many tokens was DeepSeek V3 trained on?<|im_end|>\\n<|im_start|>assistant\\nDeepSeek V3 was trained on approximately 14.8 trillion high-quality tokens. For perspective, this is comparable to Llama 3.1's 15 trillion tokens, but DeepSeek achieved much greater efficiency in its training process.<|im_end|>\\n\", \"<|im_start|>user\\nHere is information about DeepSeek V3:\\n\\nauthor - Visith Kumarapperuma Deepseek V3: A Game-Changer in A.I. Here's Why It Matters Currently, the AI models from the Chinese startup Deepseek are causing quite a stir in the AI space. Their latest reasoning model, Deepseek r1, shows better or equal performance to competitors. But above all, they achieved it with a fraction of the training and inference cost. DeepSeek's AI Assistant overtook ChatGPT to become the most downloaded free app on the U.S. App Store. This development has led to market concerns about A.I. investments to major U.S. tech companies. Impacting share prices of tech firms including Nvidia.\\n\\nBased on this information, please answer: What is DeepSeek V3 and why is it significant in the AI industry?<|im_end|>\\n<|im_start|>assistant\\nDeepSeek V3 is an AI model from the Chinese startup Deepseek that has caused a stir in the AI space due to its impressive performance at a fraction of the training and inference cost compared to competitors. It uses a Mixture-of-Experts (MoE) architecture with 671B parameters but only 37B active parameters per token, which significantly reduces compute requirements. Its significance lies in improving algorithms at the software level rather than pushing for better hardware, making AI development more efficient and potentially disrupting the industry.<|im_end|>\\n\", '<|im_start|>user\\nBased on the research about DeepSeek V3, please answer the following question: What is the HAI-LLM framework mentioned in the DeepSeek V3 paper?<|im_end|>\\n<|im_start|>assistant\\nHAI-LLM is a custom training framework developed by DeepSeek for training their V3 model. It includes several optimizations such as the DualPipe algorithm for efficient pipeline parallelism (which reduces pipeline bubbles and overlaps computation and communication), efficient cross-node all-to-all communication kernels to fully utilize network bandwidth, and careful memory optimizations to avoid using costly tensor parallelism.<|im_end|>\\n', \"<|im_start|>user\\nBased on the DeepSeek V3 research paper, Could you elaborate on what was the approximate cost of training deepseek v3 and how does it compare to other models?<|im_end|>\\n<|im_start|>assistant\\nThe approximate cost of training DeepSeek V3 was around $5.5-5.6 million, which included approximately 2.788 million GPU hours at an estimated rate of $2 per GPU hour. The training used a cluster of 2,048 H800 GPUs and was completed in less than two months. This is significantly more efficient than competitor models like Llama 3.1, which reportedly required 30.84 million GPU hours for training on a similar amount of data (15 trillion tokens vs. DeepSeek's 14.8 trillion tokens).<|im_end|>\\n\", '<|im_start|>user\\nHere is information about DeepSeek V3:\\n\\nSo what made Deepseek such a big impact to A.I. ? The significance of Deepseek as a disruptor in the industry lies in its approach. Unlike other companies that pushed for better hardware, Deepseek improved the algorithms. Thus achieving better results at a software level. Note that the following details are for the Deepseek V3 model.   Deepseek said it trained a model using a data centre of some 2,000 of Nvidia H800 GPUs.   Time duration 2 months with the cost of the  final training run being  $5.5 million This  $5.5M reflects the \"rental\" cost for the GPU hours needed to train DeepSeek V3. It does not include: The capital expenditure for owning the hardware. Costs associated with prior research, ablation studies, or experiments on alternative architectures algorithms data. Deepseek made training more efficient (45 times more efficient) Use 8-bit instead of 32-bit to save memory. Compress key value indices which eat up a lot of VRAM; they got 93% compression ratios. Do multi-token prediction instead of single-token prediction -  doubled inference speeds The MOE model decomposes a big model into small models that can run on consumer-grade hardware.\\n\\nBased on this information, please answer: How did DeepSeek V3 achieve training efficiency compared to other large language models?<|im_end|>\\n<|im_start|>assistant\\nDeepSeek V3 achieved significant training efficiency through several innovations: 1) Using a Mixture-of-Experts (MoE) architecture where only 37B parameters fire for each token out of the total 671B, 2) Implementing FP8 mixed precision training which reduced memory usage by up to 50% compared to traditional formats, 3) Developing a custom training framework called HAI-LLM with optimizations like the DualPipe algorithm for efficient pipeline parallelism, 4) Using Multi-head Latent Attention (MLA) to compress the Key-Value cache, 5) Pioneering an auxiliary loss-free strategy for load balancing in the MoE architecture, and 6) Implementing efficient cross-node communication kernels and memory optimizations.<|im_end|>\\n', '<|im_start|>user\\nBased on the DeepSeek V3 research paper, What does the research say about what is the hai-llm framework mentioned in the deepseek v3 paper?<|im_end|>\\n<|im_start|>assistant\\nHAI-LLM is a custom training framework developed by DeepSeek for training their V3 model. It includes several optimizations such as the DualPipe algorithm for efficient pipeline parallelism (which reduces pipeline bubbles and overlaps computation and communication), efficient cross-node all-to-all communication kernels to fully utilize network bandwidth, and careful memory optimizations to avoid using costly tensor parallelism.<|im_end|>\\n', '<|im_start|>user\\nBased on the research about DeepSeek V3, please answer the following question: How does DeepSeek V3 perform at reasoning and math compared to other models?<|im_end|>\\n<|im_start|>assistant\\nAccording to the document, DeepSeek V3 excels at reasoning and math tasks, surpassing the performance of GPT-4 and Claude 3.5 Sonnet in these areas. For writing and coding tasks, however, Claude 3.5 Sonnet maintains a slight lead over DeepSeek V3.<|im_end|>\\n', '<|im_start|>user\\nHere is information about DeepSeek V3:\\n\\nSo what made Deepseek such a big impact to A.I. ? The significance of Deepseek as a disruptor in the industry lies in its approach. Unlike other companies that pushed for better hardware, Deepseek improved the algorithms. Thus achieving better results at a software level. Note that the following details are for the Deepseek V3 model.   Deepseek said it trained a model using a data centre of some 2,000 of Nvidia H800 GPUs.   Time duration 2 months with the cost of the  final training run being  $5.5 million This  $5.5M reflects the \"rental\" cost for the GPU hours needed to train DeepSeek V3. It does not include: The capital expenditure for owning the hardware. Costs associated with prior research, ablation studies, or experiments on alternative architectures algorithms data. Deepseek made training more efficient (45 times more efficient) Use 8-bit instead of 32-bit to save memory. Compress key value indices which eat up a lot of VRAM; they got 93% compression ratios. Do multi-token prediction instead of single-token prediction -  doubled inference speeds The MOE model decomposes a big model into small models that can run on consumer-grade hardware.\\n\\nBased on this information, please answer: What compression ratio did DeepSeek achieve for key value indices?<|im_end|>\\n<|im_start|>assistant\\nAccording to the document, DeepSeek compressed key value indices which consume significant VRAM, achieving 93% compression ratios. This was part of their overall approach to make training more efficient.<|im_end|>\\n', \"<|im_start|>user\\nBased on the research about DeepSeek V3, please answer the following question: What was the approximate cost of training DeepSeek V3 and how does it compare to other models?<|im_end|>\\n<|im_start|>assistant\\nThe approximate cost of training DeepSeek V3 was around $5.5-5.6 million, which included approximately 2.788 million GPU hours at an estimated rate of $2 per GPU hour. The training used a cluster of 2,048 H800 GPUs and was completed in less than two months. This is significantly more efficient than competitor models like Llama 3.1, which reportedly required 30.84 million GPU hours for training on a similar amount of data (15 trillion tokens vs. DeepSeek's 14.8 trillion tokens).<|im_end|>\\n\"]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Converting train dataset to ChatML:   0%|          | 0/48 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9196fda2d04c48e792dd509f9dbc8629"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Applying chat template to train dataset:   0%|          | 0/48 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d15717557abf44008484939fadfef7da"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Tokenizing train dataset:   0%|          | 0/48 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "98041a55e37c43c0ab7a9fb594c0eadd"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Truncating train dataset:   0%|          | 0/48 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "593ceba5e8b1444da056644a8f494ca8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Debug: [{'content': 'Based on the DeepSeek V3 research paper, Please explain what is multi-head latent attention (mla) and how was it used in deepseek v3.', 'role': 'user'}, {'content': 'Multi-head Latent Attention (MLA) is a technique used in DeepSeek V3 that compresses the Key-Value cache. This reduces memory usage and enables more efficient training by decreasing the memory footprint required during model operation.', 'role': 'assistant'}] <class 'list'>\n",
            "Warning: Unexpected messages format: {'content': 'Based on the DeepSeek V3 research paper, Please explain what is multi-head latent attention (mla) and how was it used in deepseek v3.', 'role': 'user'}\n",
            "Warning: Unexpected messages format: {'content': 'Multi-head Latent Attention (MLA) is a technique used in DeepSeek V3 that compresses the Key-Value cache. This reduces memory usage and enables more efficient training by decreasing the memory footprint required during model operation.', 'role': 'assistant'}\n",
            "['', '']\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Applying formatting function to eval dataset:   0%|          | 0/6 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "160f850b209e405fae82e0dacfb966d8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Debug: [[{'content': 'Based on the DeepSeek V3 research paper, Please explain what is multi-head latent attention (mla) and how was it used in deepseek v3.', 'role': 'user'}, {'content': 'Multi-head Latent Attention (MLA) is a technique used in DeepSeek V3 that compresses the Key-Value cache. This reduces memory usage and enables more efficient training by decreasing the memory footprint required during model operation.', 'role': 'assistant'}], [{'content': \"Based on the DeepSeek V3 research paper, I'd like to know more about how many parameters does deepseek v3 have and how many are active during inference.\", 'role': 'user'}, {'content': 'DeepSeek V3 has a total of 671B parameters in its Mixture-of-Experts (MoE) architecture, but only 37B parameters are active (fire) for each token during processing. This sparse activation approach significantly reduces the computational requirements compared to dense models of similar size.', 'role': 'assistant'}], [{'content': 'Here is information about DeepSeek V3:\\n\\nFP8 Mixed Precision Training: They implemented an FP8 mixed precision training framework. Which reduces memory usage and accelerates training compared to higher precision formats. Reduced memory footprint by up to 50% compared to traditional FP16 FP32 formats. They use fine-grained quantisation strategies and increased accumulation precision to maintain accuracy.\\n\\nBased on this information, please answer: What is the HAI-LLM framework mentioned in the DeepSeek V3 paper?', 'role': 'user'}, {'content': 'HAI-LLM is a custom training framework developed by DeepSeek for training their V3 model. It includes several optimizations such as the DualPipe algorithm for efficient pipeline parallelism (which reduces pipeline bubbles and overlaps computation and communication), efficient cross-node all-to-all communication kernels to fully utilize network bandwidth, and careful memory optimizations to avoid using costly tensor parallelism.', 'role': 'assistant'}], [{'content': 'Based on the research about DeepSeek V3, please answer the following question: How does the training efficiency of DeepSeek V3 compare to Llama 3.1?', 'role': 'user'}, {'content': 'DeepSeek V3 was significantly more efficient in training compared to Llama 3.1. While DeepSeek V3 required about 2.788 million GPU hours to train on 14.8 trillion tokens, Llama 3.1 reportedly required 30.84 million GPU hours to train on a similar amount of data (15 trillion tokens). This means DeepSeek V3 was approximately 11 times more efficient than Llama 3.1 in terms of GPU hours required for training.', 'role': 'assistant'}], [{'content': 'Here is information about DeepSeek V3:\\n\\nBreakdown of the costs of the Deepseek v3 model Deepseek\\'s flagship model v3 showcases an architecture with a 671B parameter MOE (Mixture of Agents) with 37B active parameters per token Their success stems from breakthrough engineering: using MoE architecture, implementing FP8 mixed precision training, and developing a custom HAI-LLM framework. Deepseek excels at reasoning and math, surpassing GPT-4 and Claude 3.5 Sonnet. For writing and coding tasks, Claude 3.5 Sonnet maintains a slight lead. Deepseek pre-trained this model on 14.8 trillion high-quality data, taking 2,788,000 GPU hours on the Nvidia h800s cluster, costing around only $6 million the Llama 403b was trained on 11x of that, taking 30,840,000 GPU hours, also on 15 trillion tokens. So how true is the claim of $5.5 million, or is it another marketing trick? Underlying FLOP calculations Model Details: Active Parameters: 37B (using FP8 precision) FLOPs per token: Using the rule of thumb \"6 FLOPs per parameter per token.\" 37B 6   222B FLOPs per token Total Training Tokens: Approximately 14.8 trillion tokens Total FLOPs required: 222 B FLOPs token 14.8 T tokens   3.3 10²⁴ FLOPs GPU FLOP Capacity (H800 H100): An H100 is roughly estimated to deliver about. 3.958 10¹⁵ FLOPs (per second or per some standardised interval   here used as a comparative metric). Ideal (Perfect Efficiency) GPU hours. (Dividing total required FLOPs by per GPU capability gives) 3.3 10²⁴   3.958 10¹⁵    8.33 10⁸ seconds  0.4 million GPU hour Note: This \"perfect efficiency\" scenario is a lower bound. Real-world training is less efficient. 2. Adjusting for Real World Inefficiencies (Comparison with Llama 3.1) Reference Model: Llama 3.1 (405B parameters, 15 T tokens) reportedly required 30.84 M GPU hours in practice. Recalculating FLOPs for Llama 3.1: Using the same math: 3.64 10²⁵ FLOPs required Scaling Efficiency Using the ratio of FLOPs needed for DeepSeek V3 versus Llama 3.1. and assuming similar inefficiencies. The estimate adjusts to roughly 2.79M GPU hours for DeepSeek V3 training. 3. DeepSeek V3 Reported Training Breakdown According to the DeepSeek V3 paper Pre training Stage: Per Trillion Tokens: 180K H800 GPU hours Overall Pre training: Total of 2,664K GPU hours This stage was completed in less than two months using a cluster of 2,048 H800 GPUs. Context Length Extension: Additional 119K GPU hours Post training: An extra 5K GPU hours Total GPU Hours: 2,664 K 119 K 5 K 2.788M GPU hours Cost Estimation Assumed GPU Rental Price: $2 per GPU hour Total Rental Cost: 2.788M GPU hours $2 hour $5.576 million as stated in Deepseek paper During the pre training stage, training DeepSeek V3 on each trillion tokens requires only 180K H800 GPU hours  Consequently, our pre training stage is completed in less than two months and costs 2664K GPU hours. Combined with 119K GPU hours for the context length extension and 5K GPU hours for post training, DeepSeek V3 costs only 2.788M GPU hours for its full training. Assuming the rental price of the H800 GPU is $2 per GPU hour, our total training costs amount to only $5.576M. Summary Theoretical (Perfect Efficiency) Estimate:  0.4 M GPU hours (using idealised FLOP counts and assuming perfect hardware utilisation0 Adjusted (Real World) Estimate (via Llama 3.1 comparison):  2.79 GPU hours DeepSeek V3 Reported Breakdown: Pre training: 2,664K GPU hours Context Extension: 119K GPU hours Post training: 5K GPU hours Total:  2.788 M GPU hours Cost (at $2 per GPU hour):  $5.576 million\\n\\nBased on this information, please answer: How many tokens was DeepSeek V3 trained on?', 'role': 'user'}, {'content': \"DeepSeek V3 was trained on approximately 14.8 trillion high-quality tokens. For perspective, this is comparable to Llama 3.1's 15 trillion tokens, but DeepSeek achieved much greater efficiency in its training process.\", 'role': 'assistant'}], [{'content': \"Based on the DeepSeek V3 research paper, What does the research say about what are the key differences between deepseek's approach and that of other ai companies?\", 'role': 'user'}, {'content': \"The key difference in DeepSeek's approach compared to other AI companies is that while others pushed for better hardware, DeepSeek focused on improving algorithms, achieving better results at a software level. Specifically, they made training 45 times more efficient by: using 8-bit instead of 32-bit to save memory, compressing key value indices with 93% compression ratios, implementing multi-token prediction instead of single-token prediction (doubling inference speeds), and using a Mixture-of-Experts model that decomposes a big model into small models capable of running on consumer-grade hardware.\", 'role': 'assistant'}]] <class 'list'>\n",
            "['<|im_start|>user\\nBased on the DeepSeek V3 research paper, Please explain what is multi-head latent attention (mla) and how was it used in deepseek v3.<|im_end|>\\n<|im_start|>assistant\\nMulti-head Latent Attention (MLA) is a technique used in DeepSeek V3 that compresses the Key-Value cache. This reduces memory usage and enables more efficient training by decreasing the memory footprint required during model operation.<|im_end|>\\n', \"<|im_start|>user\\nBased on the DeepSeek V3 research paper, I'd like to know more about how many parameters does deepseek v3 have and how many are active during inference.<|im_end|>\\n<|im_start|>assistant\\nDeepSeek V3 has a total of 671B parameters in its Mixture-of-Experts (MoE) architecture, but only 37B parameters are active (fire) for each token during processing. This sparse activation approach significantly reduces the computational requirements compared to dense models of similar size.<|im_end|>\\n\", '<|im_start|>user\\nHere is information about DeepSeek V3:\\n\\nFP8 Mixed Precision Training: They implemented an FP8 mixed precision training framework. Which reduces memory usage and accelerates training compared to higher precision formats. Reduced memory footprint by up to 50% compared to traditional FP16 FP32 formats. They use fine-grained quantisation strategies and increased accumulation precision to maintain accuracy.\\n\\nBased on this information, please answer: What is the HAI-LLM framework mentioned in the DeepSeek V3 paper?<|im_end|>\\n<|im_start|>assistant\\nHAI-LLM is a custom training framework developed by DeepSeek for training their V3 model. It includes several optimizations such as the DualPipe algorithm for efficient pipeline parallelism (which reduces pipeline bubbles and overlaps computation and communication), efficient cross-node all-to-all communication kernels to fully utilize network bandwidth, and careful memory optimizations to avoid using costly tensor parallelism.<|im_end|>\\n', '<|im_start|>user\\nBased on the research about DeepSeek V3, please answer the following question: How does the training efficiency of DeepSeek V3 compare to Llama 3.1?<|im_end|>\\n<|im_start|>assistant\\nDeepSeek V3 was significantly more efficient in training compared to Llama 3.1. While DeepSeek V3 required about 2.788 million GPU hours to train on 14.8 trillion tokens, Llama 3.1 reportedly required 30.84 million GPU hours to train on a similar amount of data (15 trillion tokens). This means DeepSeek V3 was approximately 11 times more efficient than Llama 3.1 in terms of GPU hours required for training.<|im_end|>\\n', '<|im_start|>user\\nHere is information about DeepSeek V3:\\n\\nBreakdown of the costs of the Deepseek v3 model Deepseek\\'s flagship model v3 showcases an architecture with a 671B parameter MOE (Mixture of Agents) with 37B active parameters per token Their success stems from breakthrough engineering: using MoE architecture, implementing FP8 mixed precision training, and developing a custom HAI-LLM framework. Deepseek excels at reasoning and math, surpassing GPT-4 and Claude 3.5 Sonnet. For writing and coding tasks, Claude 3.5 Sonnet maintains a slight lead. Deepseek pre-trained this model on 14.8 trillion high-quality data, taking 2,788,000 GPU hours on the Nvidia h800s cluster, costing around only $6 million the Llama 403b was trained on 11x of that, taking 30,840,000 GPU hours, also on 15 trillion tokens. So how true is the claim of $5.5 million, or is it another marketing trick? Underlying FLOP calculations Model Details: Active Parameters: 37B (using FP8 precision) FLOPs per token: Using the rule of thumb \"6 FLOPs per parameter per token.\" 37B 6   222B FLOPs per token Total Training Tokens: Approximately 14.8 trillion tokens Total FLOPs required: 222 B FLOPs token 14.8 T tokens   3.3 10²⁴ FLOPs GPU FLOP Capacity (H800 H100): An H100 is roughly estimated to deliver about. 3.958 10¹⁵ FLOPs (per second or per some standardised interval   here used as a comparative metric). Ideal (Perfect Efficiency) GPU hours. (Dividing total required FLOPs by per GPU capability gives) 3.3 10²⁴   3.958 10¹⁵    8.33 10⁸ seconds  0.4 million GPU hour Note: This \"perfect efficiency\" scenario is a lower bound. Real-world training is less efficient. 2. Adjusting for Real World Inefficiencies (Comparison with Llama 3.1) Reference Model: Llama 3.1 (405B parameters, 15 T tokens) reportedly required 30.84 M GPU hours in practice. Recalculating FLOPs for Llama 3.1: Using the same math: 3.64 10²⁵ FLOPs required Scaling Efficiency Using the ratio of FLOPs needed for DeepSeek V3 versus Llama 3.1. and assuming similar inefficiencies. The estimate adjusts to roughly 2.79M GPU hours for DeepSeek V3 training. 3. DeepSeek V3 Reported Training Breakdown According to the DeepSeek V3 paper Pre training Stage: Per Trillion Tokens: 180K H800 GPU hours Overall Pre training: Total of 2,664K GPU hours This stage was completed in less than two months using a cluster of 2,048 H800 GPUs. Context Length Extension: Additional 119K GPU hours Post training: An extra 5K GPU hours Total GPU Hours: 2,664 K 119 K 5 K 2.788M GPU hours Cost Estimation Assumed GPU Rental Price: $2 per GPU hour Total Rental Cost: 2.788M GPU hours $2 hour $5.576 million as stated in Deepseek paper During the pre training stage, training DeepSeek V3 on each trillion tokens requires only 180K H800 GPU hours  Consequently, our pre training stage is completed in less than two months and costs 2664K GPU hours. Combined with 119K GPU hours for the context length extension and 5K GPU hours for post training, DeepSeek V3 costs only 2.788M GPU hours for its full training. Assuming the rental price of the H800 GPU is $2 per GPU hour, our total training costs amount to only $5.576M. Summary Theoretical (Perfect Efficiency) Estimate:  0.4 M GPU hours (using idealised FLOP counts and assuming perfect hardware utilisation0 Adjusted (Real World) Estimate (via Llama 3.1 comparison):  2.79 GPU hours DeepSeek V3 Reported Breakdown: Pre training: 2,664K GPU hours Context Extension: 119K GPU hours Post training: 5K GPU hours Total:  2.788 M GPU hours Cost (at $2 per GPU hour):  $5.576 million\\n\\nBased on this information, please answer: How many tokens was DeepSeek V3 trained on?<|im_end|>\\n<|im_start|>assistant\\nDeepSeek V3 was trained on approximately 14.8 trillion high-quality tokens. For perspective, this is comparable to Llama 3.1\\'s 15 trillion tokens, but DeepSeek achieved much greater efficiency in its training process.<|im_end|>\\n', \"<|im_start|>user\\nBased on the DeepSeek V3 research paper, What does the research say about what are the key differences between deepseek's approach and that of other ai companies?<|im_end|>\\n<|im_start|>assistant\\nThe key difference in DeepSeek's approach compared to other AI companies is that while others pushed for better hardware, DeepSeek focused on improving algorithms, achieving better results at a software level. Specifically, they made training 45 times more efficient by: using 8-bit instead of 32-bit to save memory, compressing key value indices with 93% compression ratios, implementing multi-token prediction instead of single-token prediction (doubling inference speeds), and using a Mixture-of-Experts model that decomposes a big model into small models capable of running on consumer-grade hardware.<|im_end|>\\n\"]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Converting eval dataset to ChatML:   0%|          | 0/6 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "46c025e84d9c42e6895a5d63c6275028"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Applying chat template to eval dataset:   0%|          | 0/6 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "dbdf845259f54d4eb824bb850892faf8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Tokenizing eval dataset:   0%|          | 0/6 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "13abb4100532474d9bf8a995c1b77a45"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Truncating eval dataset:   0%|          | 0/6 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "495cac7868c242f0afd7c3786eeb3880"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='3' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [3/3 03:37, Epoch 1/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>2.300149</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving model...\n",
            "Training complete!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1/1 : < :]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Loss: 2.3001489639282227\n",
            "Validation Perplexity: 9.975667953491211\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>█▁▁</td></tr><tr><td>eval/mean_token_accuracy</td><td>▁▇█</td></tr><tr><td>eval/runtime</td><td>█▃▁</td></tr><tr><td>eval/samples_per_second</td><td>▁▄█</td></tr><tr><td>eval/steps_per_second</td><td>▁██</td></tr><tr><td>final_val_loss</td><td>▁</td></tr><tr><td>final_val_perplexity</td><td>▁</td></tr><tr><td>train/epoch</td><td>▁███</td></tr><tr><td>train/global_step</td><td>▁████</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>2.30015</td></tr><tr><td>eval/mean_token_accuracy</td><td>0.51749</td></tr><tr><td>eval/runtime</td><td>19.0164</td></tr><tr><td>eval/samples_per_second</td><td>0.316</td></tr><tr><td>eval/steps_per_second</td><td>0.053</td></tr><tr><td>final_val_loss</td><td>2.30015</td></tr><tr><td>final_val_perplexity</td><td>9.97567</td></tr><tr><td>total_flos</td><td>433024373784576.0</td></tr><tr><td>train/epoch</td><td>1.66667</td></tr><tr><td>train/global_step</td><td>3</td></tr><tr><td>train_loss</td><td>2.8522</td></tr><tr><td>train_runtime</td><td>312.5746</td></tr><tr><td>train_samples_per_second</td><td>0.461</td></tr><tr><td>train_steps_per_second</td><td>0.01</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">dazzling-shape-5</strong> at: <a href='https://wandb.ai/myeduwebsites-university-of-moratuwa/qwen-ai-research-qa/runs/mpm4q11p' target=\"_blank\">https://wandb.ai/myeduwebsites-university-of-moratuwa/qwen-ai-research-qa/runs/mpm4q11p</a><br> View project at: <a href='https://wandb.ai/myeduwebsites-university-of-moratuwa/qwen-ai-research-qa' target=\"_blank\">https://wandb.ai/myeduwebsites-university-of-moratuwa/qwen-ai-research-qa</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250309_224351-mpm4q11p/logs</code>"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}